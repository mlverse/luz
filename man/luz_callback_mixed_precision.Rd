% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/callbacks-amp.R
\name{luz_callback_mixed_precision}
\alias{luz_callback_mixed_precision}
\title{Automatic Mixed Precision callback}
\usage{
luz_callback_mixed_precision(...)
}
\arguments{
\item{...}{Passed to \code{\link[torch:cuda_amp_grad_scaler]{torch::cuda_amp_grad_scaler()}}.}
}
\value{
A \code{luz_callback}
}
\description{
This callback will enable \code{\link[torch:local_autocast]{torch::local_autocast()}} training model forward
and during loss computation. It will then disable autocast and scale the loss
before \code{backward()} and \code{opt$step()}. See \href{https://torch.mlverse.org/docs/articles/amp.html}{here}
for more information.
}
\seealso{
Other luz_callbacks: 
\code{\link{luz_callback_auto_resume}()},
\code{\link{luz_callback_csv_logger}()},
\code{\link{luz_callback_early_stopping}()},
\code{\link{luz_callback_interrupt}()},
\code{\link{luz_callback_keep_best_model}()},
\code{\link{luz_callback_lr_scheduler}()},
\code{\link{luz_callback_metrics}()},
\code{\link{luz_callback_mixup}()},
\code{\link{luz_callback_model_checkpoint}()},
\code{\link{luz_callback_profile}()},
\code{\link{luz_callback_progress}()},
\code{\link{luz_callback_resume_from_checkpoint}()},
\code{\link{luz_callback_train_valid}()},
\code{\link{luz_callback}()}
}
\concept{luz_callbacks}
