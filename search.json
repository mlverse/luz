[{"path":"/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2021 luz authors Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"/articles/accelerator.html","id":"example","dir":"Articles","previous_headings":"","what":"Example","title":"Accelerator API","text":"Accelerator API best explained showing example diff raw torch training loop. code changes shown, longer need manually move data parameters devices, makes code easier read less error prone. can find additional documentation using help(accelerator).","code":"library(torch) + library(luz)  + acc <- accelerator() - device <- \"cpu\"  data <- tensor_dataset(   x = torch_randn(100, 10),   y = torch_rand(100, 1) )  dl <- dataloader(data, batch_size = 10)  model <- nn_linear(10, 1) - model$to(device = device) opt <- optim_adam(model$parameters)  + c(model, opt, dl) %<-% acc$prepare(model, opt, dl)  model$train() coro::loop(for (batch in dl) {    opt$zero_grad()  -  preds <- model(batch$x$to(device = device)) +  preds <- model(batch$x) -  loss <- nnf_mse_loss(preds, batch$y$to(device = device)) +  loss <- nnf_mse_loss(preds, batch$y)    loss$backward()   opt$step() })"},{"path":"/articles/checkpoints.html","id":"resuming-training-runs-that-crashed","dir":"Articles","previous_headings":"","what":"Resuming training runs that crashed","title":"Checkpointing your models","text":"long training run can crash whatever reason (computer turned , process kileed cluster, etc), recommend add luz_callback_autoresume() list callbacks. luz_callback_autoresume() automatically checkpoint whole state model end epoch. something fails training can simply rerun script, whithout code changes checkpoint reloaded training start stopped. example, lets’s take randomly generated training dataset linear model show autoresume works. ’s training data: model definition: Let’s now create callback simulates random failure happen. callback just raise R error 5th epoch. Let’s now start training adding luz_callback_auto_resume(): resume model training exactly stopped just need restart fitting, using exact model, callbacks, etc: , model fitting process continued exactly stopped. Records, optimizer model state recovered previous run can full results:","code":"x <- torch_randn(1000, 10) y <- torch_randn(1000, 1) model <- nn_linear %>%   setup(optimizer = optim_sgd, loss = nnf_mse_loss) %>%   set_hparams(in_features = 10, out_features = 1) %>%   set_opt_hparams(lr = 0.01) interrupt <- luz_callback(   \"interrupt\",   failed = FALSE,   on_epoch_end = function() {     if (ctx$epoch == 5 && !self$failed) {       self$failed <- TRUE       stop(\"Error on epoch 5\")     }   } ) autoresume <- luz_callback_auto_resume(path = \"state.pt\") inter <- interrupt()  # An error will happen in the 5th epoch and the model will be stopped. results <- model %>% fit(   list(x, y),   callbacks = list(inter, autoresume),   verbose = FALSE ) #> Error in `FUN()`: #> ! Error while calling callback with class <interrupt/LuzCallback/R6> at #>   on_epoch_end. #> Caused by error in `self[[callback_nm]]()`: #> ! Error on epoch 5 results <- model %>% fit(   list(x, y),   callbacks = list(inter, autoresume),   verbose = FALSE ) plot(results)"},{"path":"/articles/checkpoints.html","id":"checkpointing","dir":"Articles","previous_headings":"","what":"Checkpointing","title":"Checkpointing your models","text":"Sometimes want control checkpoints handled. case can use luz_callback_model_checkpoint() save checkpoints specified file directory. Let’s use example resuming section: first generate data. define model: Let’s now fit model using luz_callback_model_checkpoint(). can see now checkpoints directory contains files state dumps epoch. default, luz_callback_model_checkpoint save state epochs format name including resulting loss. can configured withing path parameter, see ?luz_callback_model_checkpoint details. Finally, can load specific checkpoint fitted result using luz_load_checkpoint. Note loading checkpoint luz_fitted_module going modify model weights -place. can start making predictions, evaluate model using reloeded weights. might also want start new training run checkpoint. , can use luz_callback_resume_from_checkpoint(). default, recover model weights checkpoint file, can configure restore records, callback optimizer state . checkpoint directory passed training resume last checkpoint file returned fs::dir_ls. ’s use callback:","code":"x <- torch_randn(1000, 10) y <- torch_randn(1000, 1) model <- nn_linear %>%   setup(optimizer = optim_sgd, loss = nnf_mse_loss) %>%   set_hparams(in_features = 10, out_features = 1) %>%   set_opt_hparams(lr = 0.01) checkpoint <- luz_callback_model_checkpoint(   path = \"checkpoints/\",    monitor = \"train_loss\" )  results <- model %>% fit(   list(x, y),   callbacks = list(checkpoint),   verbose = FALSE ) fs::dir_ls(\"checkpoints\") #> checkpoints/epoch-01-train_loss-1.237.pt #> checkpoints/epoch-02-train_loss-1.065.pt #> checkpoints/epoch-03-train_loss-1.026.pt #> checkpoints/epoch-04-train_loss-1.004.pt #> checkpoints/epoch-05-train_loss-1.004.pt #> checkpoints/epoch-06-train_loss-1.005.pt #> checkpoints/epoch-07-train_loss-0.999.pt #> checkpoints/epoch-08-train_loss-0.998.pt #> checkpoints/epoch-09-train_loss-1.001.pt #> checkpoints/epoch-10-train_loss-1.002.pt luz_load_checkpoint(results, fs::dir_ls(\"checkpoints\")[1]) resume <- luz_callback_resume_from_checkpoint(path = \"checkpoints/\") results <- model %>% fit(   list(x, y),   callbacks = list(resume),   verbose = FALSE ) plot(results)"},{"path":"/articles/checkpoints.html","id":"custom-callbacks-state","dir":"Articles","previous_headings":"Checkpointing","what":"Custom callbacks state","title":"Checkpointing your models","text":"Sometimes callbacks also need keep internal state order allow continuing training exactly stopped. case, callbacks can implement state_dict() load_state_dict() methods automatically called saving reloading checkpoints. example, suppose callback tracks gradients weights every epoch. want use tracked weights analyse training procedure. implemented like: example, gradients field state callback. training fails reason, gradients lost. ’s important also checkpoint callback state, can implement state_dict() method must returning named list objects compose state callback load_state_dict() taking named list returned state_dict() restoring callback state. callback reimplemented :","code":"cb_weight_grad <- luz_callback(   \"weight_grad\",   gradients = list(),   initialize = function(track_weights) {     self$track_weights   },   on_train_batch_before_step = function() {     gradients[[ctx$epoch]] <- list()     for (w in self$track_weights) {       gradients[[ctx$epoch]][[w]] <- self$model$parameters[[w]]     }   } ) cb_weight_grad <- luz_callback(   \"weight_grad\",   gradients = list(),   initialize = function(track_weights) {     self$track_weights   },   on_train_batch_before_step = function() {     gradients[[ctx$epoch]] <- list()     for (w in self$track_weights) {       gradients[[ctx$epoch]][[w]] <- self$model$parameters[[w]]     }   },   state_dict = function() {     list(gradients = self$gradients)   },   load_state_dict = function(d) {     self$gradients <- d$gradients   } )"},{"path":"/articles/custom-loop.html","id":"multiple-optimizers","dir":"Articles","previous_headings":"","what":"Multiple optimizers","title":"Custom loops with luz","text":"Suppose want experiment train first fully connected layer using learning rate 0.1 second one using learning rate 0.01. minimize nn_cross_entropy_loss() , first layer want add L1 regularization weights. order use luz , implement two methods net module: set_optimizers: returns named list optimizers depending ctx. loss: computes loss depending selected optimizer. Let’s go code: Notice model optimizers initialized according set_optimizers() method’s return value (list). case, initializing optimizers using different model parameters learning rates. loss() method responsible computing loss back-propagated compute gradients update weights. loss() method can access ctx object contain opt_name field, describing optimizer currently used. Note function called optimizer training validation step. See help(\"ctx\") complete information context object. can finally setup fit module, however longer need specify optimizers loss functions. Now let’s re-implement model using slightly flexible approach overriding training validation step.","code":"net <- nn_module(   \"Net\",   initialize = function() {     self$fc1 <- nn_linear(100, 50)     self$fc1 <- nn_linear(50, 10)   },   forward = function(x) {     x %>%        self$fc1() %>%        nnf_relu() %>%        self$fc2()   },   set_optimizers = function(lr_fc1 = 0.1, lr_fc2 = 0.01) {     list(       opt_fc1 = optim_adam(self$fc1$parameters, lr = lr_fc1),       opt_fc2 = optim_adam(self$fc2$parameters, lr = lr_fc2)     )   },   loss = function(input, target) {     pred <- ctx$model(input)        if (ctx$opt_name == \"opt_fc1\")        nnf_cross_entropy(pred, target) + torch_norm(self$fc1$weight, p = 1)     else if (ctx$opt_name == \"opt_fc2\")       nnf_cross_entropy(pred, target)   } ) fitted <- net %>%    setup(metrics = list(luz_metric_accuracy)) %>%    fit(train_dl, epochs = 10, valid_data = test_dl)"},{"path":"/articles/custom-loop.html","id":"fully-flexible-step","dir":"Articles","previous_headings":"","what":"Fully flexible step","title":"Custom loops with luz","text":"Instead implementing loss() method, can implement step() method. allows us flexibly modify happens training validating batch dataset. now responsible updating weights stepping optimizers back-propagating loss. important things notice : step() method used training validation. need careful modify weights training. , can get complete information regarding context object using help(\"ctx\"). ctx$optimizers named list holding optimizer created set_optimizers() method called. need manually track losses saving saving named list ctx$loss. convention, use name optimizer refers . good practice detach() saving reduce memory usage. Callbacks called inside default step() method like on_train_batch_after_pred, on_train_batch_after_loss, etc, won’t automatically called. can still cal manually adding ctx$call_callbacks(\"<callback name>\") inside training step. See code fit_one_batch() valid_one_batch find callbacks won’t called.","code":"net <- nn_module(   \"Net\",   initialize = function() {     self$fc1 <- nn_linear(100, 50)     self$fc1 <- nn_linear(50, 10)   },   forward = function(x) {     x %>%        self$fc1() %>%        nnf_relu() %>%        self$fc2()   },   set_optimizers = function(lr_fc1 = 0.1, lr_fc2 = 0.01) {     list(       opt_fc1 = optim_adam(self$fc1$parameters, lr = lr_fc1),       opt_fc2 = optim_adam(self$fc2$parameters, lr = lr_fc2)     )   },   step = function() {     ctx$loss <- list()     for (opt_name in names(ctx$optimizers)) {            pred <- ctx$model(ctx$input)       opt <- ctx$optimizers[[opt_name]]       loss <- nnf_cross_entropy(pred, target)              if (opt_name == \"opt_fc1\") {         # we have L1 regularization in layer 1         loss <- nnf_cross_entropy(pred, target) +            torch_norm(self$fc1$weight, p = 1)       }                if (ctx$training) {         opt$zero_grad()         loss$backward()         opt$step()         }              ctx$loss[[opt_name]] <- loss$detach()     }   } )"},{"path":"/articles/custom-loop.html","id":"next-steps","dir":"Articles","previous_headings":"","what":"Next steps","title":"Custom loops with luz","text":"article learned customize step() training loop using luz layered functionality. Luz also allows flexible modifications training loop described Accelerator vignette (vignette(\"accelerator\")). now able follow examples marked ‘intermediate’ ‘advanced’ category examples gallery.","code":""},{"path":"/articles/get-started.html","id":"training-a-nn_module","dir":"Articles","previous_headings":"","what":"Training a nn_module","title":"Get started with luz","text":"much possible, luz tries reuse existing structures torch. model luz defined identically define using raw torch. specific example, definition feed-forward CNN can used classify digits MNIST dataset: can now train model train_dl validate test_dl torch::dataloaders() : Let’s understand happens chunk code: setup function allows configure loss (objective) function optimizer use train model. Optionally can pass list metrics tracked training procedure. Note: loss function can function taking input target tensors returning scalar tensor value, optimizer can core torch optimizer custom ones created torch::optimizer() function. set_hparams() function allows set hyper-parameters passed module initialize() method. example case pass num_classes = 10. set_opt_hparams() function allows pass hyper-parameters used optimizer function. example, optim_adam() can take lr parameter specifying learning rate specify lr = 0.003. fit method take model specification provided setup() run training procedure using specified training validation torch::dataloaders() well number epochs. Note: reuse core torch data structures, instead providing data loading functionality. returned object fitted contains trained model well record metrics losses produced training. can also used producing predictions evaluating trained model datasets. fitting, luz use fastest possible accelerator; CUDA-capable GPU available used, otherwise fall back CPU. also automatically moves data, optimizers, models selected device don’t need handle manually (general error prone). create predictions trained model can use predict method:","code":"net <- nn_module(   \"Net\",   initialize = function(num_class) {     self$conv1 <- nn_conv2d(1, 32, 3, 1)     self$conv2 <- nn_conv2d(32, 64, 3, 1)     self$dropout1 <- nn_dropout2d(0.25)     self$dropout2 <- nn_dropout2d(0.5)     self$fc1 <- nn_linear(9216, 128)     self$fc2 <- nn_linear(128, num_class)   },   forward = function(x) {     x <- self$conv1(x)     x <- nnf_relu(x)     x <- self$conv2(x)     x <- nnf_relu(x)     x <- nnf_max_pool2d(x, 2)     x <- self$dropout1(x)     x <- torch_flatten(x, start_dim = 2)     x <- self$fc1(x)     x <- nnf_relu(x)     x <- self$dropout2(x)     x <- self$fc2(x)     x   } ) fitted <- net %>%   setup(     loss = nn_cross_entropy_loss(),     optimizer = optim_adam,     metrics = list(       luz_metric_accuracy     )   ) %>%   set_hparams(num_class = 10) %>%    set_opt_hparams(lr = 0.003) %>%    fit(train_dl, epochs = 10, valid_data = test_dl) predictions <- predict(fitted, test_dl)"},{"path":"/articles/get-started.html","id":"the-training-loop","dir":"Articles","previous_headings":"","what":"The training loop","title":"Get started with luz","text":"now general idea use fit function now ’s important overview ’s happening inside . pseudocode, ’s fit . fully detailed help build intuition:","code":"# -> Initialize objects: model, optimizers. # -> Select fitting device. # -> Move data, model, optimizers to the selected device. # -> Start training for (epoch in 1:epochs) {   # -> Training procedure   for (batch in train_dl) {     # -> Calculate model `forward` method.     # -> Calculate the loss     # -> Update weights     # -> Update metrics and tracking loss   }   # -> Validation procedure   for (batch in valid_dl) {     # -> Calculate model `forward` method.     # -> Calculate the loss     # -> Update metrics and tracking loss   } } # -> End training"},{"path":"/articles/get-started.html","id":"metrics","dir":"Articles","previous_headings":"","what":"Metrics","title":"Get started with luz","text":"One important parts machine learning projects choosing evaluation metric. Luz allows tracking many different metrics training minimal code changes. order track metrics, need modify metrics parameter setup function: Luz provides implementations used metrics. metric available can always implement new one using luz_metric function. order implement new luz_metric need implement 3 methods: initialize: defines metric initial state. function called epoch training validation loops. update: updates metric internal state. function called every training validation step predictions obtained model target values obtained dataloader. compute: uses internal state compute metric values. function called whenever need obtain current metric value. Eg, ’s called every training step metrics displayed progress bar, called per epoch record ’s value progress bar displayed. Optionally, can implement abbrev field gives metric abbreviation used displaying metric information console tracking record. abbrev passed, class name used. Let’s take look implementation luz_metric_accuracy can see implement new one: Note: ’s good practice compute metric returns regular R values instead torch tensors parts luz expect .","code":"fitted <- net %>%   setup(     ...     metrics = list(       luz_metric_accuracy     )   ) %>%   fit(...) luz_metric_accuracy <- luz_metric(   # An abbreviation to be shown in progress bars, or    # when printing progress   abbrev = \"Acc\",    # Initial setup for the metric. Metrics are initialized   # every epoch, for both training and validation   initialize = function() {     self$correct <- 0     self$total <- 0   },   # Run at every training or validation step and updates   # the internal state. The update function takes `preds`   # and `target` as parameters.   update = function(preds, target) {     pred <- torch::torch_argmax(preds, dim = 2)     self$correct <- self$correct + (pred == target)$       to(dtype = torch::torch_float())$       sum()$       item()     self$total <- self$total + pred$numel()   },   # Use the internal state to query the metric value   compute = function() {     self$correct/self$total   } )"},{"path":"/articles/get-started.html","id":"evaluate","dir":"Articles","previous_headings":"","what":"Evaluate","title":"Get started with luz","text":"model trained might want evaluate performance different dataset. reason, luz provides ?evaluate function takes fitted model dataset computes metrics attached model. Evaluate returns luz_module_evaluation object can query metrics using get_metrics function simply print see results. example:","code":"evaluation <- fitted %>% evaluate(data = valid_dl) metrics <- get_metrics(evaluation) print(evaluation) #> A `luz_module_evaluation` #> -- Results --------------------------------------------------------------------- #> loss: 1.8892 #> mae: 1.0522 #> mse: 1.645 #> rmse: 1.2826"},{"path":"/articles/get-started.html","id":"customizing-with-callbacks","dir":"Articles","previous_headings":"","what":"Customizing with callbacks","title":"Get started with luz","text":"Luz provides different ways customize training progress depending level control need training loop. fastest way ‘reusable’, sense can create training modifications can used many different situations, via callbacks. training loop luz many breakpoints can call arbitrary R functions. functionality allows customize training process without modify general training logic. Luz implements 3 default callbacks occur every training procedure: train-eval callback: Sets model train() eval() depending procedure training validation. metrics callback: evaluate metrics training validation process. progress callback: implements progress bar prints progress information training. can also implement custom callbacks modify act specifically training procedure. example: Let’s implement callback prints ‘Iteration n’ (n iteration number) every batch training set ‘Done’ epoch finished. task use luz_callback function: luz_callback() takes named functions ... arguments, name indicates moment callback called. instance on_train_batch_end() called every batch end training procedure, on_epoch_end() called end every epoch. returned value luz_callback() function initializes instance callback. Callbacks can initialization parameters, like name file want log results. case, can pass initialize method creating callback definition, save parameters self object. example, callback message parameter printed end epoch. callback defined can passed fit function via callbacks parameter: Callbacks can called many different positions training loop, including combinations . ’s overview possible callback breakpoints: Every step market on_* point training procedure available callbacks called. important part callbacks ctx (context) object. See help(\"ctx\") details. default, callbacks called order passed fit (predict evaluate), can provide weight attribute control order called. example, one callback weight = 10 another weight = 1, first one called second one. Callbacks don’t specify weight attribute considered weight = 0. built-callbacks luz already provide weight value. example, ?luz_callback_early_stopping weight Inf, since general want run last thing loop. ctx object used luz share information training loop callbacks, model methods, metrics. table describes information available ctx default. callbacks potentially modify attributes add new ones. Context attributes Attributes ctx can used produce desired behavior callbacks. can find information context object using help(\"ctx\"). example, use ctx$iter attribute print iteration number training batch.","code":"print_callback <- luz_callback(   name = \"print_callback\",   initialize = function(message) {     self$message <- message   },   on_train_batch_end = function() {     cat(\"Iteration \", ctx$iter, \"\\n\")   },   on_epoch_end = function() {     cat(self$message, \"\\n\")   } ) fitted <- net %>%   setup(...) %>%   fit(..., callbacks = list(     print_callback(message = \"Done!\")   )) Start Fit    - on_fit_begin   Start Epoch Loop      - on_epoch_begin     Start Train        - on_train_begin       Start Batch Loop          - on_train_batch_begin           Start Default Training Step             - on_train_batch_after_pred             - on_train_batch_after_loss             - on_train_batch_before_backward             - on_train_batch_before_step             - on_train_batch_after_step           End Default Training Step:          - on_train_batch_end       End Batch Loop        - on_train_end     End Train     Start Valid        - on_valid_begin       Start Batch Loop          - on_valid_batch_begin           Start Default Validation Step             - on_valid_batch_after_pred             - on_valid_batch_after_loss           End Default Validation Step          - on_valid_batch_end       End Batch Loop        - on_valid_end     End Valid       - on_epoch_end   End Epoch Loop    - on_fit_end End Fit"},{"path":"/articles/get-started.html","id":"next-steps","dir":"Articles","previous_headings":"","what":"Next steps","title":"Get started with luz","text":"article learned train first model using luz basics customization using custom metrics callbacks. Luz also allows flexible modifications training loop described vignette(\"custom-loop\"). now able follow examples marked ‘basic’ category examples gallery.","code":""},{"path":"/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Daniel Falbel. Author, maintainer, copyright holder. RStudio. Copyright holder.","code":""},{"path":"/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Falbel D (2023). luz: Higher Level 'API' 'torch'. https://mlverse.github.io/luz/, https://github.com/mlverse/luz.","code":"@Manual{,   title = {luz: Higher Level 'API' for 'torch'},   author = {Daniel Falbel},   year = {2023},   note = {https://mlverse.github.io/luz/, https://github.com/mlverse/luz}, }"},{"path":"/index.html","id":"luz","dir":"","previous_headings":"","what":"Higher Level API for torch","title":"Higher Level API for torch","text":"Luz higher level API torch providing abstractions allow much less verbose training loops. package still development. heavily inspired higher level frameworks deep learning, cite : FastAI: heavily inspired FastAI library, especially Learner object callbacks API. Keras: also heavily inspired Keras, especially callback names. lightning module interface similar compile, . PyTorch Lightning: idea luz_module subclass nn_module inspired LightningModule object lightning. HuggingFace Accelerate: internal device placement API heavily inspired Accelerate, much modest features. Currently CPU Single GPU supported.","code":""},{"path":"/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Higher Level API for torch","text":"can install released version CRAN : development version :","code":"install.packages(\"luz\") remotes::install_github(\"mlverse/luz\")"},{"path":"/index.html","id":"example","dir":"","previous_headings":"","what":"Example","title":"Higher Level API for torch","text":"Luz lets take torch nn_module definition fit dataloader, handling boring parts like moving data devices, updating weights, showing progress bars tracking metrics. ’s example defining training Autoencoder MNIST dataset. selected parts code highlight luz functionality. can find full example code . Now defined Autoencoder architecture using torch::nn_module(), can fit using luz:","code":"net <- nn_module(   \"Net\",   initialize = function() {     self$encoder <- nn_sequential(       nn_conv2d(1, 6, kernel_size=5),       nn_relu(),       nn_conv2d(6, 16, kernel_size=5),       nn_relu()     )     self$decoder <- nn_sequential(       nn_conv_transpose2d(16, 6, kernel_size = 5),       nn_relu(),       nn_conv_transpose2d(6, 1, kernel_size = 5),       nn_sigmoid()     )   },   forward = function(x) {     x %>%       self$encoder() %>%       self$decoder()   } ) fitted <- net %>%   setup(     loss = nn_mse_loss(),     optimizer = optim_adam   ) %>%   fit(train_dl, epochs = 1, valid_data = test_dl)"},{"path":"/reference/accelerator.html","id":null,"dir":"Reference","previous_headings":"","what":"Create an accelerator — accelerator","title":"Create an accelerator — accelerator","text":"Create accelerator","code":""},{"path":"/reference/accelerator.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create an accelerator — accelerator","text":"","code":"accelerator(   device_placement = TRUE,   cpu = FALSE,   cuda_index = torch::cuda_current_device() )"},{"path":"/reference/accelerator.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create an accelerator — accelerator","text":"device_placement (logical) whether accelerator object handle device placement. Default: TRUE cpu (logical) whether training procedure run CPU. cuda_index (integer) index CUDA device use multiple GPUs available. Default: result torch::cuda_current_device().","code":""},{"path":"/reference/as_dataloader.html","id":null,"dir":"Reference","previous_headings":"","what":"Creates a dataloader from its input — as_dataloader","title":"Creates a dataloader from its input — as_dataloader","text":"as_dataloader used internally luz convert input data valid_data passed fit.luz_module_generator() torch::dataloader","code":""},{"path":"/reference/as_dataloader.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Creates a dataloader from its input — as_dataloader","text":"","code":"as_dataloader(x, ...)  # S3 method for dataset as_dataloader(x, ..., batch_size = 32)  # S3 method for list as_dataloader(x, ...)  # S3 method for dataloader as_dataloader(x, ...)  # S3 method for matrix as_dataloader(x, ...)  # S3 method for numeric as_dataloader(x, ...)  # S3 method for array as_dataloader(x, ...)  # S3 method for torch_tensor as_dataloader(x, ...)"},{"path":"/reference/as_dataloader.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Creates a dataloader from its input — as_dataloader","text":"x input object. ... Passed torch::dataloader(). batch_size (int, optional): many samples per batch load (default: 1).","code":""},{"path":"/reference/as_dataloader.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Creates a dataloader from its input — as_dataloader","text":"as_dataloader methods sensible defaults batch_size, parallel workers, etc. allows users quickly experiment fit.luz_module_generator() requiring create torch::dataset torch::dataloader simple experiments.","code":""},{"path":"/reference/as_dataloader.html","id":"methods-by-class-","dir":"Reference","previous_headings":"","what":"Methods (by class)","title":"Creates a dataloader from its input — as_dataloader","text":"as_dataloader(dataset): Converts torch::dataset() torch::dataloader(). as_dataloader(list): Converts list tensors arrays size first dimension  torch::dataloader() as_dataloader(dataloader): Returns dataloader as_dataloader(matrix): Converts matrix dataloader as_dataloader(numeric): Converts numeric vector dataloader as_dataloader(array): Converts array dataloader as_dataloader(torch_tensor): Converts tensor dataloader","code":""},{"path":"/reference/as_dataloader.html","id":"overriding","dir":"Reference","previous_headings":"","what":"Overriding","title":"Creates a dataloader from its input — as_dataloader","text":"can implement as_dataloader S3 method want data structure automatically supported luz's fit.luz_module_generator(). method must satisfy following conditions: method return torch::dataloader(). required argument x. good default arguments. better avoid implementing as_dataloader methods common S3 classes like data.frames. case, better assign different class inputs implement as_dataloader .","code":""},{"path":"/reference/context.html","id":null,"dir":"Reference","previous_headings":"","what":"Context object — context","title":"Context object — context","text":"Context object storing information model training context. See also ctx.","code":""},{"path":"/reference/context.html","id":"public-fields","dir":"Reference","previous_headings":"","what":"Public fields","title":"Context object — context","text":"buffers list buffers callbacks can use write temporary information ctx.","code":""},{"path":"/reference/context.html","id":"active-bindings","dir":"Reference","previous_headings":"","what":"Active bindings","title":"Context object — context","text":"records stores information values logged self$log. device allows querying current accelerator device callbacks list callbacks called. iter current iteration batch current batch data. list input data targets. input shortcut ctx$batch[[1]] target shortcut ctx$batch[[2]] min_epochs minimum number epochs model run . max_epochs maximum number epochs model run. hparams list hyperparameters used initialize ctx$model. opt_hparams list hyperparameters used initialize ctx$optimizers. train_data dataloader used training model valid_data dataloader using model validation accelerator accelerator() used move data, model etc correct device. optimizers named list optimizers used model training. verbose bool wether process verbose mode . handlers List error handlers can used. See rlang::try_fetch() info. epoch_handlers List error handlers can used. See rlang::try_fetch() info. training bool indicating model training validation mode. model model trained. pred Last predicted values. opt Current optimizer. opt_name Current optimizer name. data Current dataloader use. loss_fn Loss function used train model loss Last computed loss values. Detached graph. loss_grad Last computed loss value, detached, can additional tranformation. epoch Current epoch. metrics List metrics tracked process.","code":""},{"path":[]},{"path":"/reference/context.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Context object — context","text":"context$new() context$log() context$log_metric() context$get_log() context$get_metrics() context$get_metric() context$get_formatted_metrics() context$get_metrics_df() context$set_verbose() context$clean() context$call_callbacks() context$state_dict() context$unsafe_set_records() context$clone()","code":""},{"path":"/reference/context.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"Context object — context","text":"Initializes context object minimal necessary information.","code":""},{"path":"/reference/context.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Context object — context","text":"","code":"context$new(verbose, accelerator, callbacks, training)"},{"path":"/reference/context.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Context object — context","text":"verbose Whether context verbose mode . accelerator luz accelerator() configures device placement others. callbacks list callbacks used model. See luz_callback(). training boolean indicates context training mode .","code":""},{"path":"/reference/context.html","id":"method-log-","dir":"Reference","previous_headings":"","what":"Method log()","title":"Context object — context","text":"Allows logging arbitrary information ctx.","code":""},{"path":"/reference/context.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Context object — context","text":"","code":"context$log(what, set, value, index = NULL, append = TRUE)"},{"path":"/reference/context.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"Context object — context","text":"(string) logging. set (string) Usually 'train' 'valid' indicating set want lot . can arbitrary info. value value log value Arbitrary value log. index Index value logged. NULL value added end list, otherwise index used. append TRUE value corresponding index already exists, value appended current value. FALSE value overwritten favor new value.","code":""},{"path":"/reference/context.html","id":"method-log-metric-","dir":"Reference","previous_headings":"","what":"Method log_metric()","title":"Context object — context","text":"Log metric gen name value. Metric values indexed epoch.","code":""},{"path":"/reference/context.html","id":"usage-2","dir":"Reference","previous_headings":"","what":"Usage","title":"Context object — context","text":"","code":"context$log_metric(name, value)"},{"path":"/reference/context.html","id":"arguments-2","dir":"Reference","previous_headings":"","what":"Arguments","title":"Context object — context","text":"name name metric value value log value Arbitrary value log.","code":""},{"path":"/reference/context.html","id":"method-get-log-","dir":"Reference","previous_headings":"","what":"Method get_log()","title":"Context object — context","text":"Get specific value log.","code":""},{"path":"/reference/context.html","id":"usage-3","dir":"Reference","previous_headings":"","what":"Usage","title":"Context object — context","text":"","code":"context$get_log(what, set, index = NULL)"},{"path":"/reference/context.html","id":"arguments-3","dir":"Reference","previous_headings":"","what":"Arguments","title":"Context object — context","text":"(string) logging. set (string) Usually 'train' 'valid' indicating set want lot . can arbitrary info. index Index value logged. NULL value added end list, otherwise index used.","code":""},{"path":"/reference/context.html","id":"method-get-metrics-","dir":"Reference","previous_headings":"","what":"Method get_metrics()","title":"Context object — context","text":"Get metric given epoch set.","code":""},{"path":"/reference/context.html","id":"usage-4","dir":"Reference","previous_headings":"","what":"Usage","title":"Context object — context","text":"","code":"context$get_metrics(set, epoch = NULL)"},{"path":"/reference/context.html","id":"arguments-4","dir":"Reference","previous_headings":"","what":"Arguments","title":"Context object — context","text":"set (string) Usually 'train' 'valid' indicating set want lot . can arbitrary info. epoch epoch want extract metrics .","code":""},{"path":"/reference/context.html","id":"method-get-metric-","dir":"Reference","previous_headings":"","what":"Method get_metric()","title":"Context object — context","text":"Get value metric given name, epoch set.","code":""},{"path":"/reference/context.html","id":"usage-5","dir":"Reference","previous_headings":"","what":"Usage","title":"Context object — context","text":"","code":"context$get_metric(name, set, epoch = NULL)"},{"path":"/reference/context.html","id":"arguments-5","dir":"Reference","previous_headings":"","what":"Arguments","title":"Context object — context","text":"name name metric set (string) Usually 'train' 'valid' indicating set want lot . can arbitrary info. epoch epoch want extract metrics .","code":""},{"path":"/reference/context.html","id":"method-get-formatted-metrics-","dir":"Reference","previous_headings":"","what":"Method get_formatted_metrics()","title":"Context object — context","text":"Get formatted metrics values","code":""},{"path":"/reference/context.html","id":"usage-6","dir":"Reference","previous_headings":"","what":"Usage","title":"Context object — context","text":"","code":"context$get_formatted_metrics(set, epoch = NULL)"},{"path":"/reference/context.html","id":"arguments-6","dir":"Reference","previous_headings":"","what":"Arguments","title":"Context object — context","text":"set (string) Usually 'train' 'valid' indicating set want lot . can arbitrary info. epoch epoch want extract metrics .","code":""},{"path":"/reference/context.html","id":"method-get-metrics-df-","dir":"Reference","previous_headings":"","what":"Method get_metrics_df()","title":"Context object — context","text":"Get data.frame containing metrics.","code":""},{"path":"/reference/context.html","id":"usage-7","dir":"Reference","previous_headings":"","what":"Usage","title":"Context object — context","text":"","code":"context$get_metrics_df()"},{"path":"/reference/context.html","id":"method-set-verbose-","dir":"Reference","previous_headings":"","what":"Method set_verbose()","title":"Context object — context","text":"Allows setting verbose attribute.","code":""},{"path":"/reference/context.html","id":"usage-8","dir":"Reference","previous_headings":"","what":"Usage","title":"Context object — context","text":"","code":"context$set_verbose(verbose = NULL)"},{"path":"/reference/context.html","id":"arguments-7","dir":"Reference","previous_headings":"","what":"Arguments","title":"Context object — context","text":"verbose boolean. TRUE verbose mode used. FALSE non verbose. NULL use result interactive().","code":""},{"path":"/reference/context.html","id":"method-clean-","dir":"Reference","previous_headings":"","what":"Method clean()","title":"Context object — context","text":"Removes unnecessary information context object.","code":""},{"path":"/reference/context.html","id":"usage-9","dir":"Reference","previous_headings":"","what":"Usage","title":"Context object — context","text":"","code":"context$clean()"},{"path":"/reference/context.html","id":"method-call-callbacks-","dir":"Reference","previous_headings":"","what":"Method call_callbacks()","title":"Context object — context","text":"Call selected callbacks. name callback types call, eg 'on_epoch_begin'.","code":""},{"path":"/reference/context.html","id":"usage-10","dir":"Reference","previous_headings":"","what":"Usage","title":"Context object — context","text":"","code":"context$call_callbacks(name)"},{"path":"/reference/context.html","id":"arguments-8","dir":"Reference","previous_headings":"","what":"Arguments","title":"Context object — context","text":"name name metric","code":""},{"path":"/reference/context.html","id":"method-state-dict-","dir":"Reference","previous_headings":"","what":"Method state_dict()","title":"Context object — context","text":"Returns list containing minimal information context. Used create returned values.","code":""},{"path":"/reference/context.html","id":"usage-11","dir":"Reference","previous_headings":"","what":"Usage","title":"Context object — context","text":"","code":"context$state_dict()"},{"path":"/reference/context.html","id":"method-unsafe-set-records-","dir":"Reference","previous_headings":"","what":"Method unsafe_set_records()","title":"Context object — context","text":"sure know ?","code":""},{"path":"/reference/context.html","id":"usage-12","dir":"Reference","previous_headings":"","what":"Usage","title":"Context object — context","text":"","code":"context$unsafe_set_records(records)"},{"path":"/reference/context.html","id":"arguments-9","dir":"Reference","previous_headings":"","what":"Arguments","title":"Context object — context","text":"records New set records set.","code":""},{"path":"/reference/context.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"Context object — context","text":"objects class cloneable method.","code":""},{"path":"/reference/context.html","id":"usage-13","dir":"Reference","previous_headings":"","what":"Usage","title":"Context object — context","text":"","code":"context$clone(deep = FALSE)"},{"path":"/reference/context.html","id":"arguments-10","dir":"Reference","previous_headings":"","what":"Arguments","title":"Context object — context","text":"deep Whether make deep clone.","code":""},{"path":"/reference/ctx.html","id":null,"dir":"Reference","previous_headings":"","what":"Context object — ctx","title":"Context object — ctx","text":"Context objects used luz share information model methods, metrics callbacks.","code":""},{"path":"/reference/ctx.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Context object — ctx","text":"ctx object used luz share information training loop callbacks, model methods, metrics. table describes information available ctx default. callbacks potentially modify attributes add new ones. Context attributes","code":""},{"path":[]},{"path":"/reference/evaluate.html","id":null,"dir":"Reference","previous_headings":"","what":"Evaluates a fitted model on a dataset — evaluate","title":"Evaluates a fitted model on a dataset — evaluate","text":"Evaluates fitted model dataset","code":""},{"path":"/reference/evaluate.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Evaluates a fitted model on a dataset — evaluate","text":"","code":"evaluate(   object,   data,   ...,   callbacks = list(),   accelerator = NULL,   verbose = NULL,   dataloader_options = NULL )"},{"path":"/reference/evaluate.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Evaluates a fitted model on a dataset — evaluate","text":"object fitted model evaluate. data (dataloader, dataset list) dataloader created torch::dataloader() used training model, dataset created torch::dataset() list. Dataloaders datasets must return list 2 items. first item used input module second used target loss function. ... Currently unused. callbacks (list, optional) list callbacks defined luz_callback() called training procedure. callbacks luz_callback_metrics(), luz_callback_progress() luz_callback_train_valid() always added default. accelerator (accelerator, optional) optional accelerator() object used configure device placement components like nn_modules, optimizers batches data. verbose (logical, optional) optional boolean value indicating fitting procedure emit output console training. default, produce output interactive() TRUE, otherwise print console. dataloader_options Options used creating dataloader. See torch::dataloader(). shuffle=TRUE default training data batch_size=32 default. error NULL data already dataloader.","code":""},{"path":"/reference/evaluate.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Evaluates a fitted model on a dataset — evaluate","text":"model trained might want evaluate performance different dataset. reason, luz provides ?evaluate function takes fitted model dataset computes metrics attached model. Evaluate returns luz_module_evaluation object can query metrics using get_metrics function simply print see results. example:","code":"evaluation <- fitted %>% evaluate(data = valid_dl) metrics <- get_metrics(evaluation) print(evaluation) ## A `luz_module_evaluation` ## -- Results --------------------------------------------------------------------- ## loss: 1.5146 ## mae: 1.0251 ## mse: 1.5159 ## rmse: 1.2312"},{"path":[]},{"path":"/reference/fit.luz_module_generator.html","id":null,"dir":"Reference","previous_headings":"","what":"Fit a nn_module — fit.luz_module_generator","title":"Fit a nn_module — fit.luz_module_generator","text":"Fit nn_module","code":""},{"path":"/reference/fit.luz_module_generator.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fit a nn_module — fit.luz_module_generator","text":"","code":"# S3 method for luz_module_generator fit(   object,   data,   epochs = 10,   callbacks = NULL,   valid_data = NULL,   accelerator = NULL,   verbose = NULL,   ...,   dataloader_options = NULL )"},{"path":"/reference/fit.luz_module_generator.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fit a nn_module — fit.luz_module_generator","text":"object nn_module setup(). data (dataloader, dataset list) dataloader created torch::dataloader() used training model, dataset created torch::dataset() list. Dataloaders datasets must return list 2 items. first item used input module second used target loss function. epochs (int) maximum number epochs training model. single value provided, taken max_epochs min_epochs set 0. vector two numbers provided, first value min_epochs second value max_epochs. minimum maximum number epochs included context object ctx$min_epochs ctx$max_epochs, respectively. callbacks (list, optional) list callbacks defined luz_callback() called training procedure. callbacks luz_callback_metrics(), luz_callback_progress() luz_callback_train_valid() always added default. valid_data (dataloader, dataset, list scalar value; optional) dataloader created torch::dataloader() dataset created torch::dataset() used validation procedure. must return list (input, target). data torch dataset list, can also supply numeric value 0 1 - case random sample size corresponding proportion data used validation. accelerator (accelerator, optional) optional accelerator() object used configure device placement components like nn_modules, optimizers batches data. verbose (logical, optional) optional boolean value indicating fitting procedure emit output console training. default, produce output interactive() TRUE, otherwise print console. ... Currently unused. dataloader_options Options used creating dataloader. See torch::dataloader(). shuffle=TRUE default training data batch_size=32 default. error NULL data already dataloader.","code":""},{"path":"/reference/fit.luz_module_generator.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fit a nn_module — fit.luz_module_generator","text":"fitted object can saved luz_save() can printed print() plotted plot().","code":""},{"path":[]},{"path":"/reference/get_metrics.html","id":null,"dir":"Reference","previous_headings":"","what":"Get metrics from the object — get_metrics","title":"Get metrics from the object — get_metrics","text":"Get metrics object","code":""},{"path":"/reference/get_metrics.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get metrics from the object — get_metrics","text":"","code":"get_metrics(object, ...)  # S3 method for luz_module_fitted get_metrics(object, ...)"},{"path":"/reference/get_metrics.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get metrics from the object — get_metrics","text":"object object query metrics. ... Currently unused.","code":""},{"path":"/reference/get_metrics.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get metrics from the object — get_metrics","text":"data.frame containing metric values.","code":""},{"path":"/reference/get_metrics.html","id":"methods-by-class-","dir":"Reference","previous_headings":"","what":"Methods (by class)","title":"Get metrics from the object — get_metrics","text":"get_metrics(luz_module_fitted): Extract metrics luz fitted model.","code":""},{"path":"/reference/lr_finder.html","id":null,"dir":"Reference","previous_headings":"","what":"Learning Rate Finder — lr_finder","title":"Learning Rate Finder — lr_finder","text":"Learning Rate Finder","code":""},{"path":"/reference/lr_finder.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Learning Rate Finder — lr_finder","text":"","code":"lr_finder(   object,   data,   steps = 100,   start_lr = 1e-07,   end_lr = 0.1,   log_spaced_intervals = TRUE,   ...,   verbose = NULL )"},{"path":"/reference/lr_finder.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Learning Rate Finder — lr_finder","text":"object nn_module setup(). data (dataloader) dataloader created torch::dataloader()  used learning rate finding. steps (integer) number steps iterate learning rate finder. Default: 100. start_lr (float) smallest learning rate. Default: 1e-7. end_lr (float) highest learning rate. Default: 1e-1. log_spaced_intervals (logical) Whether divide range start_lr end_lr log-spaced intervals (alternative: uniform intervals). Default: TRUE ... arguments passed fit. verbose Wether show progress bar process.","code":""},{"path":"/reference/lr_finder.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Learning Rate Finder — lr_finder","text":"dataframe two columns: learning rate loss","code":""},{"path":"/reference/lr_finder.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Learning Rate Finder — lr_finder","text":"","code":"if (torch::torch_is_installed()) { library(torch) ds <- torch::tensor_dataset(x = torch_randn(100, 10), y = torch_randn(100, 1)) dl <- torch::dataloader(ds, batch_size = 32) model <- torch::nn_linear model <- model %>% setup(   loss = torch::nn_mse_loss(),   optimizer = torch::optim_adam ) %>%   set_hparams(in_features = 10, out_features = 1) records <- lr_finder(model, dl, verbose = FALSE) plot(records) }"},{"path":"/reference/luz_callback.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a new callback — luz_callback","title":"Create a new callback — luz_callback","text":"Create new callback","code":""},{"path":"/reference/luz_callback.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a new callback — luz_callback","text":"","code":"luz_callback(   name = NULL,   ...,   private = NULL,   active = NULL,   parent_env = parent.frame(),   inherit = NULL )"},{"path":"/reference/luz_callback.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a new callback — luz_callback","text":"name name callback ... Public methods callback. name methods used know called. See details section. private optional list private members, can functions non-functions. active optional list active binding functions. parent_env environment use parent newly-created objects. inherit R6ClassGenerator object inherit ; words, superclass. captured unevaluated expression evaluated parent_env time object instantiated.","code":""},{"path":"/reference/luz_callback.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a new callback — luz_callback","text":"luz_callback can passed fit.luz_module_generator().","code":""},{"path":"/reference/luz_callback.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create a new callback — luz_callback","text":"Let’s implement callback prints ‘Iteration n’ (n iteration number) every batch training set ‘Done’ epoch finished. task use luz_callback function:   luz_callback() takes named functions ... arguments, name indicates moment callback called. instance on_train_batch_end() called every batch end training procedure, on_epoch_end() called end every epoch. returned value luz_callback() function initializes instance callback. Callbacks can initialization parameters, like name file want log results. case, can pass initialize method creating callback definition, save parameters self object. example, callback message parameter printed end epoch. callback defined can passed fit function via callbacks parameter:   Callbacks can called many different positions training loop, including combinations . ’s overview possible callback breakpoints:   Every step market on_* point training procedure available callbacks called. important part callbacks ctx (context) object. See help(\"ctx\") details. default, callbacks called order passed fit (predict evaluate), can provide weight attribute control order called. example, one callback weight = 10 another weight = 1, first one called second one. Callbacks don’t specify weight attribute considered weight = 0. built-callbacks luz already provide weight value. example, ?luz_callback_early_stopping weight Inf, since general want run last thing loop.","code":"print_callback <- luz_callback(   name = \"print_callback\",   initialize = function(message) {     self$message <- message   },   on_train_batch_end = function() {     cat(\"Iteration \", ctx$iter, \"\\n\")   },   on_epoch_end = function() {     cat(self$message, \"\\n\")   } ) fitted <- net %>%   setup(...) %>%   fit(..., callbacks = list(     print_callback(message = \"Done!\")   )) Start Fit    - on_fit_begin   Start Epoch Loop      - on_epoch_begin     Start Train        - on_train_begin       Start Batch Loop          - on_train_batch_begin           Start Default Training Step             - on_train_batch_after_pred             - on_train_batch_after_loss             - on_train_batch_before_backward             - on_train_batch_before_step             - on_train_batch_after_step           End Default Training Step:          - on_train_batch_end       End Batch Loop        - on_train_end     End Train     Start Valid        - on_valid_begin       Start Batch Loop          - on_valid_batch_begin           Start Default Validation Step             - on_valid_batch_after_pred             - on_valid_batch_after_loss           End Default Validation Step          - on_valid_batch_end       End Batch Loop        - on_valid_end     End Valid       - on_epoch_end   End Epoch Loop    - on_fit_end End Fit"},{"path":"/reference/luz_callback.html","id":"prediction-callbacks","dir":"Reference","previous_headings":"","what":"Prediction callbacks","title":"Create a new callback — luz_callback","text":"can also use callbacks using predict(). case supported callback methods detailed .","code":"Start predict  - on_predict_begin  Start prediction loop   - on_predict_batch_begin   - on_predict_batch_end  End prediction loop  - on_predict_end End predict"},{"path":"/reference/luz_callback.html","id":"evaluate-callbacks","dir":"Reference","previous_headings":"","what":"Evaluate callbacks","title":"Create a new callback — luz_callback","text":"Callbacks can also used evaluate(), case, callbacks used equivalent validation loop using fit():","code":"Start Valid  - on_valid_begin  Start Batch Loop   - on_valid_batch_begin   Start Default Validation Step    - on_valid_batch_after_pred    - on_valid_batch_after_loss   End Default Validation Step   - on_valid_batch_end  End Batch Loop  - on_valid_end End Valid"},{"path":[]},{"path":"/reference/luz_callback.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a new callback — luz_callback","text":"","code":"print_callback <- luz_callback(  name = \"print_callback\",  on_train_batch_end = function() {    cat(\"Iteration \", ctx$iter, \"\\n\")  },  on_epoch_end = function() {    cat(\"Done!\\n\")  } )"},{"path":"/reference/luz_callback_auto_resume.html","id":null,"dir":"Reference","previous_headings":"","what":"Resume training callback — luz_callback_auto_resume","title":"Resume training callback — luz_callback_auto_resume","text":"callback allows resume training model.","code":""},{"path":"/reference/luz_callback_auto_resume.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Resume training callback — luz_callback_auto_resume","text":"","code":"luz_callback_auto_resume(path = \"./state.pt\")"},{"path":"/reference/luz_callback_auto_resume.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Resume training callback — luz_callback_auto_resume","text":"path Path save state files model.","code":""},{"path":"/reference/luz_callback_auto_resume.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Resume training callback — luz_callback_auto_resume","text":"using , model weights, optimizer state serialized end epoch. something fails training simply re-running script restart model training epoch right last epoch serialized.","code":""},{"path":"/reference/luz_callback_auto_resume.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Resume training callback — luz_callback_auto_resume","text":"general want add callback last callbacks list, way, serialized state likely contain possible changes callbacks made 'on_epoch_end'. default weight attribute callback Inf. Read checkpointing article pkgdown website information.","code":""},{"path":"/reference/luz_callback_auto_resume.html","id":"customizing-serialization","dir":"Reference","previous_headings":"","what":"Customizing serialization","title":"Resume training callback — luz_callback_auto_resume","text":"default model, optimizer state records serialized. Callbacks can used customize serialization implementing state_dict() load_state_dict() methods. methods implemented, state_dict() called end epoch load_state_dict() called model resumed.","code":""},{"path":[]},{"path":"/reference/luz_callback_auto_resume.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Resume training callback — luz_callback_auto_resume","text":"","code":"if (torch::torch_is_installed()) { library(torch) library(luz)  x <- torch_randn(1000, 10) y <- torch_randn(1000, 1)  model <- nn_linear %>%   setup(optimizer = optim_sgd, loss = nnf_mse_loss) %>%   set_hparams(in_features = 10, out_features = 1) %>%   set_opt_hparams(lr = 0.01)   # simulate a failure in the middle of epoch 5 happening only once. callback_stop <- luz_callback(   \"interrupt\",   failed = FALSE,   on_epoch_end = function() {     if (ctx$epoch == 5 && !self$failed) {       self$failed <- TRUE       stop(\"Error on epoch 5\")     }   } )  path <- tempfile() autoresume <- luz_callback_auto_resume(path = path) interrupt <- callback_stop()  # try once and the model fails try({   results <- model %>% fit(     list(x, y),     callbacks = list(autoresume, interrupt),     verbose = FALSE   ) })  # model resumes and completes results <- model %>% fit(   list(x, y),   callbacks = list(autoresume, interrupt),   verbose = FALSE )  get_metrics(results)  } #> Error in FUN(X[[i]], ...) :  #>   Error while calling callback with class <interrupt/LuzCallback/R6> at #> on_epoch_end. #> Caused by error in `self[[callback_nm]]()`: #> ! Error on epoch 5 #>      set metric epoch     value #> 1  train   loss     1 1.2192523 #> 2  train   loss     2 1.0346544 #> 3  train   loss     3 0.9827294 #> 4  train   loss     4 0.9743557 #> 5  train   loss     5 0.9663411 #> 6  train   loss     6 0.9686206 #> 7  train   loss     7 0.9676100 #> 8  train   loss     8 0.9637525 #> 9  train   loss     9 0.9644775 #> 10 train   loss    10 0.9694295"},{"path":"/reference/luz_callback_csv_logger.html","id":null,"dir":"Reference","previous_headings":"","what":"CSV logger callback — luz_callback_csv_logger","title":"CSV logger callback — luz_callback_csv_logger","text":"Logs metrics obtained training fiel disk. file 1 line epoch/validation.","code":""},{"path":"/reference/luz_callback_csv_logger.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"CSV logger callback — luz_callback_csv_logger","text":"","code":"luz_callback_csv_logger(path)"},{"path":"/reference/luz_callback_csv_logger.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"CSV logger callback — luz_callback_csv_logger","text":"path path file disk.","code":""},{"path":[]},{"path":"/reference/luz_callback_early_stopping.html","id":null,"dir":"Reference","previous_headings":"","what":"Early stopping callback — luz_callback_early_stopping","title":"Early stopping callback — luz_callback_early_stopping","text":"Stops training monitored metric stops improving","code":""},{"path":"/reference/luz_callback_early_stopping.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Early stopping callback — luz_callback_early_stopping","text":"","code":"luz_callback_early_stopping(   monitor = \"valid_loss\",   min_delta = 0,   patience = 0,   mode = \"min\",   baseline = NULL )"},{"path":"/reference/luz_callback_early_stopping.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Early stopping callback — luz_callback_early_stopping","text":"monitor string format <set>_<metric> <set> can 'train' 'valid' <metric> can abbreviation metric tracking training. metric name case insensitive. min_delta Minimum improvement reset patience counter. patience Number epochs without improving stoping training. mode Specifies direction considered improvement. default 'min' used. Can also 'max' (higher better) 'zero' (closer zero better). baseline initial value used best seen value begining. Model stopm training better baseline value found first patience epochs.","code":""},{"path":"/reference/luz_callback_early_stopping.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Early stopping callback — luz_callback_early_stopping","text":"luz_callback early stopping.","code":""},{"path":"/reference/luz_callback_early_stopping.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Early stopping callback — luz_callback_early_stopping","text":"callback adds on_early_stopping callback can used call callbacks soon model stops training. verbose=TRUE fit.luz_module_generator() message printed early stopping.","code":""},{"path":[]},{"path":"/reference/luz_callback_early_stopping.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Early stopping callback — luz_callback_early_stopping","text":"","code":"cb <- luz_callback_early_stopping()"},{"path":"/reference/luz_callback_gradient_clip.html","id":null,"dir":"Reference","previous_headings":"","what":"Gradient clipping callback — luz_callback_gradient_clip","title":"Gradient clipping callback — luz_callback_gradient_clip","text":"adding GradientClip callback, gradient norm_type (default:2) norm clipped max_norm (default:1) using torch::nn_utils_clip_grad_norm_(), can avoid loss divergence.","code":""},{"path":"/reference/luz_callback_gradient_clip.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Gradient clipping callback — luz_callback_gradient_clip","text":"","code":"luz_callback_gradient_clip(max_norm = 1, norm_type = 2)"},{"path":"/reference/luz_callback_gradient_clip.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Gradient clipping callback — luz_callback_gradient_clip","text":"max_norm (float int): max norm gradients norm_type (float int): type used p-norm. Can Inf infinity norm.","code":""},{"path":"/reference/luz_callback_gradient_clip.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Gradient clipping callback — luz_callback_gradient_clip","text":"See FastAI documentation GradientClip callback.","code":""},{"path":"/reference/luz_callback_interrupt.html","id":null,"dir":"Reference","previous_headings":"","what":"Interrupt callback — luz_callback_interrupt","title":"Interrupt callback — luz_callback_interrupt","text":"Adds handler allows interrupting training loop using ctrl + C. Also registers on_interrupt breakpoint users can register callbacks run training loop interruption.","code":""},{"path":"/reference/luz_callback_interrupt.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Interrupt callback — luz_callback_interrupt","text":"","code":"luz_callback_interrupt()"},{"path":"/reference/luz_callback_interrupt.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Interrupt callback — luz_callback_interrupt","text":"luz_callback","code":""},{"path":"/reference/luz_callback_interrupt.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Interrupt callback — luz_callback_interrupt","text":"general need use callback always included default fit.luz_module_generator().","code":""},{"path":[]},{"path":"/reference/luz_callback_interrupt.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Interrupt callback — luz_callback_interrupt","text":"","code":"interrupt_callback <- luz_callback_interrupt()"},{"path":"/reference/luz_callback_keep_best_model.html","id":null,"dir":"Reference","previous_headings":"","what":"Keep the best model — luz_callback_keep_best_model","title":"Keep the best model — luz_callback_keep_best_model","text":"epoch, improvement monitored metric serialize model weights temp file. training done, reload weights best model.","code":""},{"path":"/reference/luz_callback_keep_best_model.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Keep the best model — luz_callback_keep_best_model","text":"","code":"luz_callback_keep_best_model(   monitor = \"valid_loss\",   mode = \"min\",   min_delta = 0 )"},{"path":"/reference/luz_callback_keep_best_model.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Keep the best model — luz_callback_keep_best_model","text":"monitor string format <set>_<metric> <set> can 'train' 'valid' <metric> can abbreviation metric tracking training. metric name case insensitive. mode Specifies direction considered improvement. default 'min' used. Can also 'max' (higher better) 'zero' (closer zero better). min_delta Minimum improvement reset patience counter.","code":""},{"path":[]},{"path":"/reference/luz_callback_keep_best_model.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Keep the best model — luz_callback_keep_best_model","text":"","code":"cb <- luz_callback_keep_best_model()"},{"path":"/reference/luz_callback_lr_scheduler.html","id":null,"dir":"Reference","previous_headings":"","what":"Learning rate scheduler callback — luz_callback_lr_scheduler","title":"Learning rate scheduler callback — luz_callback_lr_scheduler","text":"Initializes runs torch::lr_scheduler()s.","code":""},{"path":"/reference/luz_callback_lr_scheduler.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Learning rate scheduler callback — luz_callback_lr_scheduler","text":"","code":"luz_callback_lr_scheduler(   lr_scheduler,   ...,   call_on = \"on_epoch_end\",   opt_name = NULL )"},{"path":"/reference/luz_callback_lr_scheduler.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Learning rate scheduler callback — luz_callback_lr_scheduler","text":"lr_scheduler torch::lr_scheduler() initialized optimizer ... parameters. ... Additional arguments passed lr_scheduler together optimizers. call_on callback breakpoint scheduler$step() called. Default 'on_epoch_end'. See luz_callback() information. opt_name name optimizer affected callback. match name given set_optimizers. module single optimizer, opt_name used.","code":""},{"path":"/reference/luz_callback_lr_scheduler.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Learning rate scheduler callback — luz_callback_lr_scheduler","text":"luz_callback() generator.","code":""},{"path":[]},{"path":"/reference/luz_callback_lr_scheduler.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Learning rate scheduler callback — luz_callback_lr_scheduler","text":"","code":"if (torch::torch_is_installed()) { cb <- luz_callback_lr_scheduler(torch::lr_step, step_size = 30) }"},{"path":"/reference/luz_callback_metrics.html","id":null,"dir":"Reference","previous_headings":"","what":"Metrics callback — luz_callback_metrics","title":"Metrics callback — luz_callback_metrics","text":"Tracks metrics passed setup() training validation.","code":""},{"path":"/reference/luz_callback_metrics.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Metrics callback — luz_callback_metrics","text":"","code":"luz_callback_metrics()"},{"path":"/reference/luz_callback_metrics.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Metrics callback — luz_callback_metrics","text":"luz_callback","code":""},{"path":"/reference/luz_callback_metrics.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Metrics callback — luz_callback_metrics","text":"callback takes care 2 ctx attributes: ctx$metrics: stores current metrics objects initialized epoch, update()d compute()d every batch. rarely need work metrics. ctx$records$metrics: Stores metrics per training/validation epoch. structure similar ctx$losses.","code":""},{"path":"/reference/luz_callback_metrics.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Metrics callback — luz_callback_metrics","text":"general need explicitly use metrics callback used default fit.luz_module_generator().","code":""},{"path":[]},{"path":"/reference/luz_callback_mixup.html","id":null,"dir":"Reference","previous_headings":"","what":"Mixup callback — luz_callback_mixup","title":"Mixup callback — luz_callback_mixup","text":"Implementation 'mixup: Beyond Empirical Risk Minimization'. today, tested categorical data, targets expected integers, one-hot encoded vectors. callback supposed used together nn_mixup_loss().","code":""},{"path":"/reference/luz_callback_mixup.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Mixup callback — luz_callback_mixup","text":"","code":"luz_callback_mixup(alpha = 0.4, ..., run_valid = FALSE, auto_loss = FALSE)"},{"path":"/reference/luz_callback_mixup.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Mixup callback — luz_callback_mixup","text":"alpha parameter beta distribution used sample mixing coefficients ... currently unused. Just force named arguments. run_valid run validation auto_loss automatically modify loss function? wrap loss function create mixup loss. TRUE make sure loss function apply reductions. run_valid=FALSE, loss mean reduced validation.","code":""},{"path":"/reference/luz_callback_mixup.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Mixup callback — luz_callback_mixup","text":"luz_callback","code":""},{"path":"/reference/luz_callback_mixup.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Mixup callback — luz_callback_mixup","text":"Overall, follow fastai implementation described . Namely, work single dataloader , randomly mixing two observations batch. linearly combine losses computed targets: loss(output, new_target) = weight * loss(output, target1) + (1-weight) * loss(output, target2) draw different mixing coefficients every pair. replace weight weight = max(weight, 1-weight) avoid duplicates.","code":""},{"path":[]},{"path":"/reference/luz_callback_mixup.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Mixup callback — luz_callback_mixup","text":"","code":"if (torch::torch_is_installed()) { mixup_callback <- luz_callback_mixup() }"},{"path":"/reference/luz_callback_model_checkpoint.html","id":null,"dir":"Reference","previous_headings":"","what":"Checkpoints model weights — luz_callback_model_checkpoint","title":"Checkpoints model weights — luz_callback_model_checkpoint","text":"saves checkpoints model according specified metric behavior.","code":""},{"path":"/reference/luz_callback_model_checkpoint.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Checkpoints model weights — luz_callback_model_checkpoint","text":"","code":"luz_callback_model_checkpoint(   path,   monitor = \"valid_loss\",   save_best_only = FALSE,   mode = \"min\",   min_delta = 0 )"},{"path":"/reference/luz_callback_model_checkpoint.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Checkpoints model weights — luz_callback_model_checkpoint","text":"path Path save model disk. path interpolated glue, can use attribute within ctx using '{ctx$epoch}'. Specially epoch monitor quantities already environment. specified path path directory (ends / \\), models saved name given epoch-{epoch:02d}-{self$monitor}-{monitor:.3f}.pt. See examples. can use sprintf() quickly format quantities, example:'{epoch:02d}'. monitor string format <set>_<metric> <set> can 'train' 'valid' <metric> can abbreviation metric tracking training. metric name case insensitive. save_best_only TRUE models saved improvement previously saved model. mode Specifies direction considered improvement. default 'min' used. Can also 'max' (higher better) 'zero' (closer zero better). min_delta Minimum difference consider improvement. used save_best_only=TRUE.","code":""},{"path":"/reference/luz_callback_model_checkpoint.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Checkpoints model weights — luz_callback_model_checkpoint","text":"mode min_delta used save_best_only=TRUE. save_best_only overwrite saved models path parameter differentiate epochs. Read checkpointing article pkgdown website information.","code":""},{"path":[]},{"path":"/reference/luz_callback_model_checkpoint.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Checkpoints model weights — luz_callback_model_checkpoint","text":"","code":"luz_callback_model_checkpoint(path= \"path/to/dir\") #> <model_checkpoint_callback> #>   Inherits from: <monitor_metrics> #>   Public: #>     call: function (callback_nm)  #>     clone: function (deep = FALSE)  #>     compare: function (new, old)  #>     find_quantity: function ()  #>     fmt_path: function (path)  #>     initialize: function ()  #>     min_delta: 0 #>     mode: min #>     monitor: valid_loss #>     on_epoch_end: function ()  #>     path: path/to/dir #>     save_best_only: FALSE #>     set_ctx: function (ctx)  luz_callback_model_checkpoint(path= \"path/to/dir/epoch-{epoch:02d}/model.pt\") #> <model_checkpoint_callback> #>   Inherits from: <monitor_metrics> #>   Public: #>     call: function (callback_nm)  #>     clone: function (deep = FALSE)  #>     compare: function (new, old)  #>     find_quantity: function ()  #>     fmt_path: function (path)  #>     initialize: function ()  #>     min_delta: 0 #>     mode: min #>     monitor: valid_loss #>     on_epoch_end: function ()  #>     path: path/to/dir/epoch-{epoch:02d}/model.pt #>     save_best_only: FALSE #>     set_ctx: function (ctx)  luz_callback_model_checkpoint(path= \"path/to/dir/epoch-{epoch:02d}/model-{monitor:.2f}.pt\") #> <model_checkpoint_callback> #>   Inherits from: <monitor_metrics> #>   Public: #>     call: function (callback_nm)  #>     clone: function (deep = FALSE)  #>     compare: function (new, old)  #>     find_quantity: function ()  #>     fmt_path: function (path)  #>     initialize: function ()  #>     min_delta: 0 #>     mode: min #>     monitor: valid_loss #>     on_epoch_end: function ()  #>     path: path/to/dir/epoch-{epoch:02d}/model-{monitor:.2f}.pt #>     save_best_only: FALSE #>     set_ctx: function (ctx)"},{"path":"/reference/luz_callback_profile.html","id":null,"dir":"Reference","previous_headings":"","what":"Profile callback — luz_callback_profile","title":"Profile callback — luz_callback_profile","text":"Computes times high-level operations training loops.","code":""},{"path":"/reference/luz_callback_profile.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Profile callback — luz_callback_profile","text":"","code":"luz_callback_profile()"},{"path":"/reference/luz_callback_profile.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Profile callback — luz_callback_profile","text":"luz_callback","code":""},{"path":"/reference/luz_callback_profile.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Profile callback — luz_callback_profile","text":"Records saved ctx$records$profile. Times stored seconds. Data stored following structure: fit time entire fit procedure. epoch times per epoch (train/valid)_batch time per batch data processed, including data acquisition step. (train/valid)_step time per step (training validation step) - model step. (including data acquisition preprocessing)","code":""},{"path":"/reference/luz_callback_profile.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Profile callback — luz_callback_profile","text":"general need use callback always included default fit.luz_module_generator().","code":""},{"path":[]},{"path":"/reference/luz_callback_profile.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Profile callback — luz_callback_profile","text":"","code":"profile_callback <- luz_callback_profile()"},{"path":"/reference/luz_callback_progress.html","id":null,"dir":"Reference","previous_headings":"","what":"Progress callback — luz_callback_progress","title":"Progress callback — luz_callback_progress","text":"Responsible printing progress training.","code":""},{"path":"/reference/luz_callback_progress.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Progress callback — luz_callback_progress","text":"","code":"luz_callback_progress()"},{"path":"/reference/luz_callback_progress.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Progress callback — luz_callback_progress","text":"luz_callback","code":""},{"path":"/reference/luz_callback_progress.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Progress callback — luz_callback_progress","text":"general need use callback always included default fit.luz_module_generator(). Printing can disabled passing verbose=FALSE fit.luz_module_generator().","code":""},{"path":[]},{"path":"/reference/luz_callback_resume_from_checkpoint.html","id":null,"dir":"Reference","previous_headings":"","what":"Allow resume model training from a specific checkpoint — luz_callback_resume_from_checkpoint","title":"Allow resume model training from a specific checkpoint — luz_callback_resume_from_checkpoint","text":"Allow resume model training specific checkpoint","code":""},{"path":"/reference/luz_callback_resume_from_checkpoint.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Allow resume model training from a specific checkpoint — luz_callback_resume_from_checkpoint","text":"","code":"luz_callback_resume_from_checkpoint(   path,   ...,   restore_model_state = TRUE,   restore_records = FALSE,   restore_optimizer_state = FALSE,   restore_callbacks_state = FALSE )"},{"path":"/reference/luz_callback_resume_from_checkpoint.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Allow resume model training from a specific checkpoint — luz_callback_resume_from_checkpoint","text":"path Path checkpoint want resume. ... currently unused. restore_model_state Wether restore model state callback. restore_records Wether restore records checkpoint. restore_optimizer_state Wether restore optimizer state checkpoint. restore_callbacks_state Wether restore callbacks state checkpoint.","code":""},{"path":"/reference/luz_callback_resume_from_checkpoint.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Allow resume model training from a specific checkpoint — luz_callback_resume_from_checkpoint","text":"Read checkpointing article pkgdown website information.","code":""},{"path":[]},{"path":"/reference/luz_callback_train_valid.html","id":null,"dir":"Reference","previous_headings":"","what":"Train-eval callback — luz_callback_train_valid","title":"Train-eval callback — luz_callback_train_valid","text":"Switches important flags training evaluation modes.","code":""},{"path":"/reference/luz_callback_train_valid.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Train-eval callback — luz_callback_train_valid","text":"","code":"luz_callback_train_valid()"},{"path":"/reference/luz_callback_train_valid.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Train-eval callback — luz_callback_train_valid","text":"luz_callback","code":""},{"path":"/reference/luz_callback_train_valid.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Train-eval callback — luz_callback_train_valid","text":"takes care three ctx attributes: ctx$model: Responsible calling ctx$model$train() ctx$model$eval(), appropriate. ctx$training: Sets flag TRUE training FALSE validation mode. ctx$loss: Resets loss attribute list() finished training/ validating.","code":""},{"path":"/reference/luz_callback_train_valid.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Train-eval callback — luz_callback_train_valid","text":"general need explicitly use metrics callback used default fit.luz_module_generator().","code":""},{"path":[]},{"path":"/reference/luz_load.html","id":null,"dir":"Reference","previous_headings":"","what":"Load trained model — luz_load","title":"Load trained model — luz_load","text":"Loads fitted model. See documentation luz_save().","code":""},{"path":"/reference/luz_load.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Load trained model — luz_load","text":"","code":"luz_load(path)"},{"path":"/reference/luz_load.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Load trained model — luz_load","text":"path path file system save object.","code":""},{"path":[]},{"path":"/reference/luz_load_checkpoint.html","id":null,"dir":"Reference","previous_headings":"","what":"Loads a checkpoint — luz_load_checkpoint","title":"Loads a checkpoint — luz_load_checkpoint","text":"Works checkpoints created typically luz_callback_model_checkpoint().","code":""},{"path":"/reference/luz_load_checkpoint.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Loads a checkpoint — luz_load_checkpoint","text":"","code":"luz_load_checkpoint(obj, path, ...)"},{"path":"/reference/luz_load_checkpoint.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Loads a checkpoint — luz_load_checkpoint","text":"obj Object want laod checkpoint. path Path checkpoint disk. ... unused. allow future extensions.","code":""},{"path":"/reference/luz_load_model_weights.html","id":null,"dir":"Reference","previous_headings":"","what":"Loads model weights into a fitted object. — luz_load_model_weights","title":"Loads model weights into a fitted object. — luz_load_model_weights","text":"can useful saved model checkpoints training want reload best checkpoint end.","code":""},{"path":"/reference/luz_load_model_weights.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Loads model weights into a fitted object. — luz_load_model_weights","text":"","code":"luz_load_model_weights(obj, path, ...)  luz_save_model_weights(obj, path)"},{"path":"/reference/luz_load_model_weights.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Loads model weights into a fitted object. — luz_load_model_weights","text":"obj luz object want copy new weights. path path saved model disk. ... arguments passed torch_load().","code":""},{"path":"/reference/luz_load_model_weights.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Loads model weights into a fitted object. — luz_load_model_weights","text":"Returns NULL invisibly.","code":""},{"path":"/reference/luz_load_model_weights.html","id":"warning","dir":"Reference","previous_headings":"","what":"Warning","title":"Loads model weights into a fitted object. — luz_load_model_weights","text":"luz_save_model_weights operates inplace, ie modifies model object contain new weights.","code":""},{"path":"/reference/luz_metric.html","id":null,"dir":"Reference","previous_headings":"","what":"Creates a new luz metric — luz_metric","title":"Creates a new luz metric — luz_metric","text":"Creates new luz metric","code":""},{"path":"/reference/luz_metric.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Creates a new luz metric — luz_metric","text":"","code":"luz_metric(   name = NULL,   ...,   private = NULL,   active = NULL,   parent_env = parent.frame(),   inherit = NULL )"},{"path":"/reference/luz_metric.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Creates a new luz metric — luz_metric","text":"name string naming new metric. ... named list public methods. implement least initialize, update compute. See details section information. private optional list private members, can functions non-functions. active optional list active binding functions. parent_env environment use parent newly-created objects. inherit R6ClassGenerator object inherit ; words, superclass. captured unevaluated expression evaluated parent_env time object instantiated.","code":""},{"path":"/reference/luz_metric.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Creates a new luz metric — luz_metric","text":"Returns new luz metric.","code":""},{"path":"/reference/luz_metric.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Creates a new luz metric — luz_metric","text":"order implement new luz_metric need implement 3 methods: initialize: defines metric initial state. function called epoch training validation loops. update: updates metric internal state. function called every training validation step predictions obtained model target values obtained dataloader. compute: uses internal state compute metric values. function called whenever need obtain current metric value. Eg, ’s called every training step metrics displayed progress bar, called per epoch record ’s value progress bar displayed. Optionally, can implement abbrev field gives metric abbreviation used displaying metric information console tracking record. abbrev passed, class name used. Let’s take look implementation luz_metric_accuracy can see implement new one:   Note: ’s good practice compute metric returns regular R values instead torch tensors parts luz expect .","code":"luz_metric_accuracy <- luz_metric(   # An abbreviation to be shown in progress bars, or    # when printing progress   abbrev = \"Acc\",    # Initial setup for the metric. Metrics are initialized   # every epoch, for both training and validation   initialize = function() {     self$correct <- 0     self$total <- 0   },   # Run at every training or validation step and updates   # the internal state. The update function takes `preds`   # and `target` as parameters.   update = function(preds, target) {     pred <- torch::torch_argmax(preds, dim = 2)     self$correct <- self$correct + (pred == target)$       to(dtype = torch::torch_float())$       sum()$       item()     self$total <- self$total + pred$numel()   },   # Use the internal state to query the metric value   compute = function() {     self$correct/self$total   } )"},{"path":[]},{"path":"/reference/luz_metric.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Creates a new luz metric — luz_metric","text":"","code":"luz_metric_accuracy <- luz_metric(   # An abbreviation to be shown in progress bars, or   # when printing progress   abbrev = \"Acc\",   # Initial setup for the metric. Metrics are initialized   # every epoch, for both training and validation   initialize = function() {     self$correct <- 0     self$total <- 0   },   # Run at every training or validation step and updates   # the internal state. The update function takes `preds`   # and `target` as parameters.   update = function(preds, target) {     pred <- torch::torch_argmax(preds, dim = 2)     self$correct <- self$correct + (pred == target)$       to(dtype = torch::torch_float())$       sum()$       item()     self$total <- self$total + pred$numel()   },   # Use the internal state to query the metric value   compute = function() {     self$correct/self$total   } )"},{"path":"/reference/luz_metric_accuracy.html","id":null,"dir":"Reference","previous_headings":"","what":"Accuracy — luz_metric_accuracy","title":"Accuracy — luz_metric_accuracy","text":"Computes accuracy multi-class classification problems.","code":""},{"path":"/reference/luz_metric_accuracy.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Accuracy — luz_metric_accuracy","text":"","code":"luz_metric_accuracy()"},{"path":"/reference/luz_metric_accuracy.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Accuracy — luz_metric_accuracy","text":"Returns new luz metric.","code":""},{"path":"/reference/luz_metric_accuracy.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Accuracy — luz_metric_accuracy","text":"metric expects take logits probabilities every update. take columnwise argmax compare target.","code":""},{"path":[]},{"path":"/reference/luz_metric_accuracy.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Accuracy — luz_metric_accuracy","text":"","code":"if (torch::torch_is_installed()) { library(torch) metric <- luz_metric_accuracy() metric <- metric$new() metric$update(torch_randn(100, 10), torch::torch_randint(1, 10, size = 100)) metric$compute() } #> [1] 0.11"},{"path":"/reference/luz_metric_binary_accuracy.html","id":null,"dir":"Reference","previous_headings":"","what":"Binary accuracy — luz_metric_binary_accuracy","title":"Binary accuracy — luz_metric_binary_accuracy","text":"Computes accuracy binary classification problems model returns probabilities. Commonly used loss torch::nn_bce_loss().","code":""},{"path":"/reference/luz_metric_binary_accuracy.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Binary accuracy — luz_metric_binary_accuracy","text":"","code":"luz_metric_binary_accuracy(threshold = 0.5)"},{"path":"/reference/luz_metric_binary_accuracy.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Binary accuracy — luz_metric_binary_accuracy","text":"threshold value used classifiy observations 0 1.","code":""},{"path":"/reference/luz_metric_binary_accuracy.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Binary accuracy — luz_metric_binary_accuracy","text":"Returns new luz metric.","code":""},{"path":[]},{"path":"/reference/luz_metric_binary_accuracy.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Binary accuracy — luz_metric_binary_accuracy","text":"","code":"if (torch::torch_is_installed()) { library(torch) metric <- luz_metric_binary_accuracy(threshold = 0.5) metric <- metric$new() metric$update(torch_rand(100), torch::torch_randint(0, 1, size = 100)) metric$compute() } #> [1] 0.44"},{"path":"/reference/luz_metric_binary_accuracy_with_logits.html","id":null,"dir":"Reference","previous_headings":"","what":"Binary accuracy with logits — luz_metric_binary_accuracy_with_logits","title":"Binary accuracy with logits — luz_metric_binary_accuracy_with_logits","text":"Computes accuracy binary classification problems model return logits. Commonly used together torch::nn_bce_with_logits_loss().","code":""},{"path":"/reference/luz_metric_binary_accuracy_with_logits.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Binary accuracy with logits — luz_metric_binary_accuracy_with_logits","text":"","code":"luz_metric_binary_accuracy_with_logits(threshold = 0.5)"},{"path":"/reference/luz_metric_binary_accuracy_with_logits.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Binary accuracy with logits — luz_metric_binary_accuracy_with_logits","text":"threshold value used classifiy observations 0 1.","code":""},{"path":"/reference/luz_metric_binary_accuracy_with_logits.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Binary accuracy with logits — luz_metric_binary_accuracy_with_logits","text":"Returns new luz metric.","code":""},{"path":"/reference/luz_metric_binary_accuracy_with_logits.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Binary accuracy with logits — luz_metric_binary_accuracy_with_logits","text":"Probabilities generated using torch::nnf_sigmoid() threshold used classify 0 1.","code":""},{"path":[]},{"path":"/reference/luz_metric_binary_accuracy_with_logits.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Binary accuracy with logits — luz_metric_binary_accuracy_with_logits","text":"","code":"if (torch::torch_is_installed()) { library(torch) metric <- luz_metric_binary_accuracy_with_logits(threshold = 0.5) metric <- metric$new() metric$update(torch_randn(100), torch::torch_randint(0, 1, size = 100)) metric$compute() } #> [1] 0.56"},{"path":"/reference/luz_metric_binary_auroc.html","id":null,"dir":"Reference","previous_headings":"","what":"Computes the area under the ROC — luz_metric_binary_auroc","title":"Computes the area under the ROC — luz_metric_binary_auroc","text":"avoid storing predictions targets epoch compute confusion matrices across range pre-established thresholds.","code":""},{"path":"/reference/luz_metric_binary_auroc.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Computes the area under the ROC — luz_metric_binary_auroc","text":"","code":"luz_metric_binary_auroc(   num_thresholds = 200,   thresholds = NULL,   from_logits = FALSE )"},{"path":"/reference/luz_metric_binary_auroc.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Computes the area under the ROC — luz_metric_binary_auroc","text":"num_thresholds Number thresholds used compute confusion matrices. case, thresholds created getting num_thresholds values linearly spaced unit interval. thresholds (optional) threshold passed, used compute confusion matrices num_thresholds ignored. from_logits Boolean indicating predictions logits, case use sigmoid put unit interval.","code":""},{"path":[]},{"path":"/reference/luz_metric_binary_auroc.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Computes the area under the ROC — luz_metric_binary_auroc","text":"","code":"if (torch::torch_is_installed()){ library(torch) actual <- c(1, 1, 1, 0, 0, 0) predicted <- c(0.9, 0.8, 0.4, 0.5, 0.3, 0.2)  y_true <- torch_tensor(actual) y_pred <- torch_tensor(predicted)  m <- luz_metric_binary_auroc(thresholds = predicted) m <- m$new()  m$update(y_pred[1:2], y_true[1:2]) m$update(y_pred[3:4], y_true[3:4]) m$update(y_pred[5:6], y_true[5:6])  m$compute() } #> [1] 0.8888889"},{"path":"/reference/luz_metric_mae.html","id":null,"dir":"Reference","previous_headings":"","what":"Mean absolute error — luz_metric_mae","title":"Mean absolute error — luz_metric_mae","text":"Computes mean absolute error.","code":""},{"path":"/reference/luz_metric_mae.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Mean absolute error — luz_metric_mae","text":"","code":"luz_metric_mae()"},{"path":"/reference/luz_metric_mae.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Mean absolute error — luz_metric_mae","text":"Returns new luz metric.","code":""},{"path":[]},{"path":"/reference/luz_metric_mae.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Mean absolute error — luz_metric_mae","text":"","code":"if (torch::torch_is_installed()) { library(torch) metric <- luz_metric_mae() metric <- metric$new() metric$update(torch_randn(100), torch_randn(100)) metric$compute() } #> [1] 1.025279"},{"path":"/reference/luz_metric_mse.html","id":null,"dir":"Reference","previous_headings":"","what":"Mean squared error — luz_metric_mse","title":"Mean squared error — luz_metric_mse","text":"Computes mean squared error","code":""},{"path":"/reference/luz_metric_mse.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Mean squared error — luz_metric_mse","text":"","code":"luz_metric_mse()"},{"path":"/reference/luz_metric_mse.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Mean squared error — luz_metric_mse","text":"luz_metric object.","code":""},{"path":[]},{"path":"/reference/luz_metric_multiclass_auroc.html","id":null,"dir":"Reference","previous_headings":"","what":"Computes the multi-class AUROC — luz_metric_multiclass_auroc","title":"Computes the multi-class AUROC — luz_metric_multiclass_auroc","text":"definition Keras used default. equivalent 'micro' method SciKit Learn . See docs.","code":""},{"path":"/reference/luz_metric_multiclass_auroc.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Computes the multi-class AUROC — luz_metric_multiclass_auroc","text":"","code":"luz_metric_multiclass_auroc(   num_thresholds = 200,   thresholds = NULL,   from_logits = FALSE,   average = c(\"micro\", \"macro\", \"weighted\", \"none\") )"},{"path":"/reference/luz_metric_multiclass_auroc.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Computes the multi-class AUROC — luz_metric_multiclass_auroc","text":"num_thresholds Number thresholds used compute confusion matrices. case, thresholds created getting num_thresholds values linearly spaced unit interval. thresholds (optional) threshold passed, used compute confusion matrices num_thresholds ignored. from_logits TRUE call torch::nnf_softmax() predictions computing metric. average averaging method: 'micro': Stack classes computes AUROC binary classification problem. 'macro': Finds AUCROC class computes mean. 'weighted': Finds AUROC class computes weighted mean pondering number instances class. 'none': Returns AUROC class list.","code":""},{"path":"/reference/luz_metric_multiclass_auroc.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Computes the multi-class AUROC — luz_metric_multiclass_auroc","text":"Note class imbalance can affect metric unlike AUC binary classification. Currently AUC approximated using 'interpolation' method described Keras.","code":""},{"path":[]},{"path":"/reference/luz_metric_multiclass_auroc.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Computes the multi-class AUROC — luz_metric_multiclass_auroc","text":"","code":"if (torch::torch_is_installed()) { library(torch) actual <- c(1, 1, 1, 0, 0, 0) + 1L predicted <- c(0.9, 0.8, 0.4, 0.5, 0.3, 0.2) predicted <- cbind(1-predicted, predicted)  y_true <- torch_tensor(as.integer(actual)) y_pred <- torch_tensor(predicted)  m <- luz_metric_multiclass_auroc(thresholds = as.numeric(predicted),                                  average = \"micro\") m <- m$new()  m$update(y_pred[1:2,], y_true[1:2]) m$update(y_pred[3:4,], y_true[3:4]) m$update(y_pred[5:6,], y_true[5:6]) m$compute() } #> [1] 0.9027778"},{"path":"/reference/luz_metric_rmse.html","id":null,"dir":"Reference","previous_headings":"","what":"Root mean squared error — luz_metric_rmse","title":"Root mean squared error — luz_metric_rmse","text":"Computes root mean squared error.","code":""},{"path":"/reference/luz_metric_rmse.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Root mean squared error — luz_metric_rmse","text":"","code":"luz_metric_rmse()"},{"path":"/reference/luz_metric_rmse.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Root mean squared error — luz_metric_rmse","text":"Returns new luz metric.","code":""},{"path":[]},{"path":"/reference/luz_metric_set.html","id":null,"dir":"Reference","previous_headings":"","what":"Creates a metric set — luz_metric_set","title":"Creates a metric set — luz_metric_set","text":"metric set can used specify metrics evaluated training, validation .","code":""},{"path":"/reference/luz_metric_set.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Creates a metric set — luz_metric_set","text":"","code":"luz_metric_set(metrics = NULL, train_metrics = NULL, valid_metrics = NULL)"},{"path":"/reference/luz_metric_set.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Creates a metric set — luz_metric_set","text":"metrics list luz_metrics meant used training validation. train_metrics list luz_metrics used training. valid_metrics list luz_metrics sued validation.","code":""},{"path":"/reference/luz_save.html","id":null,"dir":"Reference","previous_headings":"","what":"Saves luz objects to disk — luz_save","title":"Saves luz objects to disk — luz_save","text":"Allows saving luz fitted models disk. Objects can loaded back luz_load().","code":""},{"path":"/reference/luz_save.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Saves luz objects to disk — luz_save","text":"","code":"luz_save(obj, path, ...)"},{"path":"/reference/luz_save.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Saves luz objects to disk — luz_save","text":"obj object class 'luz_module_fitted' returned fit.luz_module_generator(). path path file system save object. ... currently unused.","code":""},{"path":"/reference/luz_save.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Saves luz objects to disk — luz_save","text":"Objects saved plain .rds files obj$model serialized torch_save saving .","code":""},{"path":"/reference/luz_save.html","id":"warning","dir":"Reference","previous_headings":"","what":"Warning","title":"Saves luz objects to disk — luz_save","text":"ctx naively serialized. Ie, use saveRDS() serialize . expect luz_save work correctly unserializable objects ctx like torch_tensors external pointers general.","code":""},{"path":[]},{"path":"/reference/nn_mixup_loss.html","id":null,"dir":"Reference","previous_headings":"","what":"Loss to be used with callbacks_mixup(). — nn_mixup_loss","title":"Loss to be used with callbacks_mixup(). — nn_mixup_loss","text":"training phase, computes individual losses regard two targets, weights item-wise, averages linear combinations yield mean batch loss. validation testing, defers passed-loss.","code":""},{"path":"/reference/nn_mixup_loss.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Loss to be used with callbacks_mixup(). — nn_mixup_loss","text":"","code":"nn_mixup_loss(loss)"},{"path":"/reference/nn_mixup_loss.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Loss to be used with callbacks_mixup(). — nn_mixup_loss","text":"loss underlying loss nn_module call. must support reduction field. training attribute changed 'none' get loss individual observations. See example documentation reduction argument torch::nn_cross_entropy_loss().","code":""},{"path":"/reference/nn_mixup_loss.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Loss to be used with callbacks_mixup(). — nn_mixup_loss","text":"used together luz_callback_mixup().","code":""},{"path":[]},{"path":"/reference/nnf_mixup.html","id":null,"dir":"Reference","previous_headings":"","what":"Mixup logic — nnf_mixup","title":"Mixup logic — nnf_mixup","text":"Logic underlying luz_callback_mixup().","code":""},{"path":"/reference/nnf_mixup.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Mixup logic — nnf_mixup","text":"","code":"nnf_mixup(x, y, weight)"},{"path":"/reference/nnf_mixup.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Mixup logic — nnf_mixup","text":"x input batch y target batch weight weighting coefficient used torch_lerp()","code":""},{"path":"/reference/nnf_mixup.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Mixup logic — nnf_mixup","text":"list : x, new, mixed-input batch y, list : ys, list : y1, original target y1 y2, mixed-target y2 weight, mixing weights","code":""},{"path":"/reference/nnf_mixup.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Mixup logic — nnf_mixup","text":"Based passed-input target batches, well applicable mixing weights, return new tensors intended replace current batch. new input batch weighted linear combination input batch items, new target batch bundles original targets, well mixing weights, nested list.","code":""},{"path":[]},{"path":"/reference/nnf_mixup.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Mixup logic — nnf_mixup","text":"","code":"if (torch::torch_is_installed()) { batch_x <- torch::torch_randn(c(10, 768)) batch_y <- torch::torch_randn(10) weight <- torch::torch_tensor(rep(0.9, 10))$view(c(10, 1)) nnf_mixup(batch_x, batch_y, weight) } #> $x #> torch_tensor #> Columns 1 to 10 0.0471 -0.3960  0.1831 -1.3533  0.8482 -0.3397  0.2712  0.2076  0.6414 -0.3867 #> -1.1847 -0.4114  1.6663  1.6568  1.6280  0.6788  0.4035  1.4302 -0.3156  1.1555 #> -0.1444 -0.1544  0.9179 -2.3987  1.4588 -0.9783 -0.7982  0.0952 -0.5717 -0.7066 #>  0.2920  1.9798 -0.7682  0.1725  0.9738  0.9840  0.8623  1.0153 -1.0577  1.7361 #>  0.2379 -1.0642 -1.5048 -0.2829 -0.9043  0.6475 -0.6551 -0.6280  1.0132  0.6255 #>  0.4308  1.7266  0.1520 -0.3705 -1.1746  0.2547 -0.0598  0.0227  1.6180 -0.6547 #>  0.1892  0.2331  1.3495  1.2575  0.6889  0.6204  0.6986  0.3958 -0.6146  0.2813 #>  0.2405 -0.5854  0.5224 -0.1970  0.3656  0.9756  0.6754  1.3868  0.1776  0.5066 #> -0.9542 -0.9670 -0.1713 -0.7556  2.1828  0.5050 -0.8082 -0.3513 -0.2268 -0.7296 #> -0.6593  0.4649 -1.4361  1.5572  0.6728 -0.2841  1.3680  0.2478 -0.1393 -0.1782 #>  #> Columns 11 to 20 0.4217 -0.4263 -0.7064  0.0872  0.7933 -1.4815  0.8983  0.0833 -0.5042 -0.4328 #> -0.6651 -1.1458 -0.2817  0.2328 -1.1159  0.0540  0.9313 -1.0396 -1.0475 -0.4261 #> -0.9094 -0.6527 -0.3066 -1.5690 -0.1850 -0.1579 -0.4476 -1.0572  0.4411  0.5723 #> -1.7139  1.4749 -1.0576  1.1793 -0.6476  0.0919  1.2532 -0.9785  0.6205 -0.4211 #> -0.1768 -0.8604  0.6973 -0.2501 -1.3664  0.9249  1.2895  2.5905  1.2916  1.3818 #> -1.1209 -0.6283 -0.1723 -0.1147 -0.4707 -0.4402 -0.0368  1.0924 -0.2243  0.7058 #>  0.2826 -0.2471 -0.8957  0.5307 -0.3570  1.7517 -0.1657 -0.0284 -0.8001  2.2384 #>  0.1538  1.5186  0.8812  1.2411  0.7741  0.3825  0.0517 -0.1919  2.2015  1.0135 #> -0.4136 -0.5627 -0.2856  0.2589  1.2913 -0.0482  1.5316 -1.1487 -0.5348  0.1740 #> -0.5969 -0.6000  0.2033  0.4452  0.9769  1.3474  1.0047  0.1087  0.7992 -1.0555 #>  #> Columns 21 to 30-0.1935  0.5565 -0.6298 -0.3984  0.1182  0.2037  0.1195  1.5663 -1.2078 -0.2108 #> -0.5918  1.6134  1.5566  0.6819 -0.1198  0.1461 -0.4163 -2.1621 -0.6545  0.5421 #> -0.1395 -0.7489  1.8462 -0.8655 -0.6889 -0.8857 -0.3398  1.0896 -1.3994 -0.8709 #> -0.2663 -1.0177 -0.9308 -0.3023 -0.5663  0.5708  0.2384 -0.5835 -0.4456  1.0058 #>  0.2796 -1.4203  2.2239 -1.5310  0.5873  0.8132 -0.7452 -0.4338 -0.6509 -0.8671 #> -0.8411 -1.3950  1.3048 -1.6133  0.3291  0.0785  0.1777  0.3471  0.0881  0.3414 #> -0.1221  0.1552 -2.6603 -0.4103 -3.4103  0.6510 -1.0009  0.5235 -1.1094 -0.0936 #> -0.5031 -0.6861  0.1147 -0.8525 -0.8737 -0.0919  0.9933  1.2692 -1.1781  0.2487 #> ... [the output was truncated (use n=-1 to disable)] #> [ CPUFloatType{10,768} ] #>  #> $y #> $y$ys #> $y$ys$y1 #> torch_tensor #> -2.4131 #> -0.7169 #> -0.4950 #>  0.2414 #> -0.3643 #>  0.5187 #>  0.3147 #>  2.1676 #> -0.9031 #> -0.8404 #> [ CPUFloatType{10} ] #>  #> $y$ys$y2 #> torch_tensor #> -0.4950 #>  2.1676 #> -0.9031 #> -0.8404 #> -2.4131 #> -0.3643 #>  0.5187 #> -0.7169 #>  0.3147 #>  0.2414 #> [ CPUFloatType{10} ] #>  #>  #> $y$weight #> torch_tensor #>  0.9000 #>  0.9000 #>  0.9000 #>  0.9000 #>  0.9000 #>  0.9000 #>  0.9000 #>  0.9000 #>  0.9000 #>  0.9000 #> [ CPUFloatType{10,1} ] #>  #>"},{"path":"/reference/pipe.html","id":null,"dir":"Reference","previous_headings":"","what":"Pipe operator — %>%","title":"Pipe operator — %>%","text":"See magrittr::%>% details.","code":""},{"path":"/reference/pipe.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Pipe operator — %>%","text":"","code":"lhs %>% rhs"},{"path":"/reference/predict.luz_module_fitted.html","id":null,"dir":"Reference","previous_headings":"","what":"Create predictions for a fitted model — predict.luz_module_fitted","title":"Create predictions for a fitted model — predict.luz_module_fitted","text":"Create predictions fitted model","code":""},{"path":"/reference/predict.luz_module_fitted.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create predictions for a fitted model — predict.luz_module_fitted","text":"","code":"# S3 method for luz_module_fitted predict(   object,   newdata,   ...,   callbacks = list(),   accelerator = NULL,   verbose = NULL,   dataloader_options = NULL )"},{"path":"/reference/predict.luz_module_fitted.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create predictions for a fitted model — predict.luz_module_fitted","text":"object (fitted model) fitted model object returned fit.luz_module_generator() newdata (dataloader, dataset, list array) returning list least 1 element. elements used. ... Currently unused. callbacks (list, optional) list callbacks defined luz_callback() called training procedure. callbacks luz_callback_metrics(), luz_callback_progress() luz_callback_train_valid() always added default. accelerator (accelerator, optional) optional accelerator() object used configure device placement components like nn_modules, optimizers batches data. verbose (logical, optional) optional boolean value indicating fitting procedure emit output console training. default, produce output interactive() TRUE, otherwise print console. dataloader_options Options used creating dataloader. See torch::dataloader(). shuffle=TRUE default training data batch_size=32 default. error NULL data already dataloader.","code":""},{"path":[]},{"path":"/reference/reexports.html","id":null,"dir":"Reference","previous_headings":"","what":"Objects exported from other packages — reexports","title":"Objects exported from other packages — reexports","text":"objects imported packages. Follow links see documentation. generics fit","code":""},{"path":"/reference/set_hparams.html","id":null,"dir":"Reference","previous_headings":"","what":"Set hyper-parameter of a module — set_hparams","title":"Set hyper-parameter of a module — set_hparams","text":"function used define hyper-parameters calling fit luz_modules.","code":""},{"path":"/reference/set_hparams.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Set hyper-parameter of a module — set_hparams","text":"","code":"set_hparams(module, ...)"},{"path":"/reference/set_hparams.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Set hyper-parameter of a module — set_hparams","text":"module nn_module setup(). ... parameters set used initialize nn_module, ie passed unchanged initialize method base nn_module.","code":""},{"path":"/reference/set_hparams.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Set hyper-parameter of a module — set_hparams","text":"luz module","code":""},{"path":[]},{"path":"/reference/set_opt_hparams.html","id":null,"dir":"Reference","previous_headings":"","what":"Set optimizer hyper-parameters — set_opt_hparams","title":"Set optimizer hyper-parameters — set_opt_hparams","text":"function used define hyper-parameters optimizer initialization method.","code":""},{"path":"/reference/set_opt_hparams.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Set optimizer hyper-parameters — set_opt_hparams","text":"","code":"set_opt_hparams(module, ...)"},{"path":"/reference/set_opt_hparams.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Set optimizer hyper-parameters — set_opt_hparams","text":"module nn_module setup(). ... parameters passed used initialize optimizers. example, optimizer optim_adam pass lr=0.1, optim_adam function called optim_adam(parameters, lr=0.1) fitting model.","code":""},{"path":"/reference/set_opt_hparams.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Set optimizer hyper-parameters — set_opt_hparams","text":"luz module","code":""},{"path":[]},{"path":"/reference/setup.html","id":null,"dir":"Reference","previous_headings":"","what":"Set's up a nn_module to use with luz — setup","title":"Set's up a nn_module to use with luz — setup","text":"setup function used set important attributes method nn_modules used luz.","code":""},{"path":"/reference/setup.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Set's up a nn_module to use with luz — setup","text":"","code":"setup(module, loss = NULL, optimizer = NULL, metrics = NULL, backward = NULL)"},{"path":"/reference/setup.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Set's up a nn_module to use with luz — setup","text":"module (nn_module) nn_module want set . loss (function, optional) optional function signature function(input, target). requires nn_module implement method called loss. optimizer (torch_optimizer, optional) function signature function(parameters, ...) used initialize optimizer given model parameters. metrics (list, optional) list metrics tracked training procedure. Sometimes, want metrics evaluated training validation, case can pass luz_metric_set() object specify mmetrics used stage. backward (function) functions takes loss scalar values parameter. must call $backward() torch::autograd_backward(). general need set parameter unless need customize luz calls backward(), example, need add additional arguments backward call. Note becomes method nn_module thus can used custom step() override .","code":""},{"path":"/reference/setup.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Set's up a nn_module to use with luz — setup","text":"luz module can trained fit().","code":""},{"path":"/reference/setup.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Set's up a nn_module to use with luz — setup","text":"makes sure module necessary ingredients order fitted.","code":""},{"path":"/reference/setup.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Set's up a nn_module to use with luz — setup","text":"also adds device active field can used query current module device within methods, eg self$device. useful ctx() available, eg, calling methods outside luz wrappers. Users can override default implementing device active method input module.","code":""},{"path":[]},{"path":"/news/index.html","id":"luz-development-version","dir":"Changelog","previous_headings":"","what":"luz (development version)","title":"luz (development version)","text":"Added support arm Mac’s MPS device. (#104) Refactor checkpointing luz - now also serialize optimizer state callbacks state. (#107) Added luz_callback_autoresume() allowing easily resume trainining runs might crashed. (#107) Added th luz_callback_resume_from_checkpoint() allowing one resume training run checkpoint file. (#107) Users can now chose metrics called training validation, training validation. See luz_metric_set() information. (#112) Improved errors raised user code, eg calling metrics callbacks raised. helps lot debuging errors callbacks metrics. (#112) loss_fn now field context, thus callbacks can override needed. (#112) luz_callback_mixup now supports run_valid auto_loss arguments. (#112) ctx now aliases default opt opt_name single optimizer specified (ie. cases) (#114)","code":""},{"path":"/news/index.html","id":"luz-031","dir":"Changelog","previous_headings":"","what":"luz 0.3.1","title":"luz 0.3.1","text":"CRAN release: 2022-09-06 Re-submission fix vignette rendering.","code":""},{"path":"/news/index.html","id":"luz-030","dir":"Changelog","previous_headings":"","what":"luz 0.3.0","title":"luz 0.3.0","text":"CRAN release: 2022-08-19","code":""},{"path":"/news/index.html","id":"breaking-changes-0-3-0","dir":"Changelog","previous_headings":"","what":"Breaking changes","title":"luz 0.3.0","text":"lr_finder() now default divides range start_lr end_lr log-spaced intervals, following fast.ai implementation. Cf. Sylvain Gugger’s post: https://sgugger.github.io/---find--good-learning-rate.html. previous behavior can achieved passing log_spaced_intervals=FALSE function. (#82, @skeydan) plot.lr_records() now addition plots exponentially weighted moving average loss (, see Sylvain Gugger’s post), weighting coefficient 0.9 (seems reasonable value default setting 100 learning-rate-incrementing intervals). (#82, @skeydan)","code":""},{"path":"/news/index.html","id":"documentation-0-3-0","dir":"Changelog","previous_headings":"","what":"Documentation","title":"luz 0.3.0","text":"Many wording improvements getting started guides (#81 #94, @jonthegeek).","code":""},{"path":"/news/index.html","id":"new-features-0-3-0","dir":"Changelog","previous_headings":"","what":"New features","title":"luz 0.3.0","text":"Added MixUp callback helper loss function functional logic. (#82, @skeydan). Added luz_callback_gradient_clip inspired FastAI’s implementation. (#90) Added backward argument setup allowing one customize backward called loss scalar value. (#93) Added luz_callback_keep_best_model() reload weights best model training finished. (#95)","code":""},{"path":"/news/index.html","id":"luz-020","dir":"Changelog","previous_headings":"","what":"luz 0.2.0","title":"luz 0.2.0","text":"CRAN release: 2021-10-07","code":""},{"path":"/news/index.html","id":"new-features-0-2-0","dir":"Changelog","previous_headings":"","what":"New features","title":"luz 0.2.0","text":"Allow users provide minimum maximum number epochs calling fit.luz_module_generator(). Removed ctx$epochs context object replaced ctx$min_epochs ctx$max_epochs (#53, @mattwarkentin). Early stopping now occur minimum number training epochs met (#53, @mattwarkentin). Added cuda_index argument accelerator allow selecting specific GPU multiple present (#58, @cmcmaster1). Implemented lr_finder (#59, @cmcmaster1). now handle different kinds data arguments passed fit using as_dataloader() method (#66). valid_data can now scalar value indicating proportion data used fitting. works data torch dataset list. (#69) can now supply dataloader_options fit pass additional information as_dataloader(). (#71) Implemented evaluate function allowing users get metrics model new dataset. (#73)","code":""},{"path":"/news/index.html","id":"bug-fixes-0-2-0","dir":"Changelog","previous_headings":"","what":"Bug fixes","title":"luz 0.2.0","text":"Fixed bug CSV logger callback saving logs space delimited file (#52, @mattwarkentin). Fixed bug length progress bar validation dataset (#52, @mattwarkentin). Fixed bugs early stopping callback related working properly patience = 1 specified logging callbacks. (#76)","code":""},{"path":"/news/index.html","id":"internal-changes-0-2-0","dir":"Changelog","previous_headings":"","what":"Internal changes","title":"luz 0.2.0","text":"ctx$data now refers current use data instead always refering ctx$train_data. (#54) Refactored ctx object make safer avoid returing output. (#73)","code":""},{"path":"/news/index.html","id":"luz-010","dir":"Changelog","previous_headings":"","what":"luz 0.1.0","title":"luz 0.1.0","text":"CRAN release: 2021-06-17 Added NEWS.md file track changes package.","code":""}]
